{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KIIT0001\\ocr\\ocrd\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model2 = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.54 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.145  Python-3.7.3 torch-1.13.1+cpu CPU (12th Gen Intel Core(TM) i7-1255U)\n",
      "WARNING  Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data2.yaml, epochs=50, patience=50, batch=16, imgsz=346, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train5\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011238 parameters, 3011222 gradients\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "WARNING  imgsz=[346] must be multiple of max stride 32, updating to [352]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\KIIT0001\\ocr\\ocr_id\\samples\\new_preprocessed\\train\\labels.cache... 335 images, 9 backgrounds, 0 corrupt: 100%|██████████| 344/344 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\KIIT0001\\ocr\\ocr_id\\samples\\new_preprocessed\\valid\\labels.cache... 9 images, 0 backgrounds, 0 corrupt: 100%|██████████| 9/9 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train5\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 352 train, 352 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train5\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/50         0G      1.308       2.68       1.04         34        352: 100%|██████████| 22/22 [00:45<00:00,  2.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "                   all          9         17          1      0.325      0.455      0.345\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/50         0G      1.072       1.21     0.9487         37        352: 100%|██████████| 22/22 [00:47<00:00,  2.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "                   all          9         17          1      0.255      0.471      0.372\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/50         0G      1.097       1.04     0.9595         27        352: 100%|██████████| 22/22 [00:48<00:00,  2.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
      "                   all          9         17          1      0.277      0.819      0.625\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/50         0G      1.058     0.9514     0.9626         26        352: 100%|██████████| 22/22 [00:49<00:00,  2.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n",
      "                   all          9         17      0.835      0.514      0.831      0.645\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/50         0G      1.022      0.885     0.9445         36        352: 100%|██████████| 22/22 [00:51<00:00,  2.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      "                   all          9         17      0.828      0.651       0.86      0.643\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/50         0G      1.034     0.9218      0.961         18        352: 100%|██████████| 22/22 [00:47<00:00,  2.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "                   all          9         17       0.89      0.819      0.866      0.617\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/50         0G     0.9974     0.8227     0.9479         26        352: 100%|██████████| 22/22 [00:47<00:00,  2.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.50it/s]\n",
      "                   all          9         17      0.935      0.882      0.952      0.719\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/50         0G     0.9738     0.8492     0.9395         21        352: 100%|██████████| 22/22 [00:46<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "                   all          9         17      0.927      0.944      0.954      0.671\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/50         0G     0.9627     0.8094      0.938         21        352: 100%|██████████| 22/22 [00:58<00:00,  2.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "                   all          9         17      0.915      0.874      0.937      0.743\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/50         0G      1.001     0.8209     0.9396         24        352: 100%|██████████| 22/22 [00:51<00:00,  2.34s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "                   all          9         17      0.904      0.944      0.937      0.778\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/50         0G     0.9681     0.7811      0.932         21        352: 100%|██████████| 22/22 [00:51<00:00,  2.34s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "                   all          9         17      0.993      0.928      0.949      0.729\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/50         0G     0.9365     0.7673     0.9236         28        352: 100%|██████████| 22/22 [00:57<00:00,  2.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "                   all          9         17      0.981      0.941      0.955       0.75\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/50         0G     0.9277     0.7061     0.9243         29        352: 100%|██████████| 22/22 [00:48<00:00,  2.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "                   all          9         17      0.968      0.944      0.965      0.743\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/50         0G     0.8873     0.6671     0.8989         27        352: 100%|██████████| 22/22 [00:47<00:00,  2.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.00it/s]\n",
      "                   all          9         17      0.921      0.944      0.945      0.777\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/50         0G     0.8824     0.6399     0.9093         21        352: 100%|██████████| 22/22 [00:49<00:00,  2.26s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.45it/s]\n",
      "                   all          9         17      0.913      0.944      0.947      0.747\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/50         0G     0.8887      0.641     0.9039         15        352: 100%|██████████| 22/22 [00:48<00:00,  2.22s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      "                   all          9         17       0.92      0.944      0.962      0.742\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/50         0G     0.8851     0.6358     0.9043         27        352: 100%|██████████| 22/22 [00:47<00:00,  2.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "                   all          9         17      0.935      0.944      0.958      0.758\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/50         0G     0.8945     0.6568     0.9224         25        352: 100%|██████████| 22/22 [00:48<00:00,  2.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "                   all          9         17      0.916      0.944      0.952      0.775\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/50         0G     0.8636     0.6115     0.9079         30        352: 100%|██████████| 22/22 [00:47<00:00,  2.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "                   all          9         17      0.904      0.944      0.955      0.756\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/50         0G     0.8617     0.6355     0.8999         19        352: 100%|██████████| 22/22 [00:53<00:00,  2.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n",
      "                   all          9         17      0.907      0.944      0.959      0.779\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/50         0G     0.8265     0.5855     0.8836         31        352: 100%|██████████| 22/22 [03:16<00:00,  8.94s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "                   all          9         17      0.915      0.944      0.964      0.775\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/50         0G     0.8625      0.614     0.8961         30        352: 100%|██████████| 22/22 [00:45<00:00,  2.06s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.16it/s]\n",
      "                   all          9         17      0.925      0.944      0.957      0.751\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/50         0G     0.8352      0.585      0.903         18        352: 100%|██████████| 22/22 [00:47<00:00,  2.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "                   all          9         17      0.908      0.944       0.95      0.783\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/50         0G     0.8407     0.5835        0.9         26        352: 100%|██████████| 22/22 [00:48<00:00,  2.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "                   all          9         17      0.907      0.944      0.933      0.754\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/50         0G      0.824     0.5843     0.8929         18        352: 100%|██████████| 22/22 [00:47<00:00,  2.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "                   all          9         17      0.913      0.944      0.919      0.743\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      26/50         0G     0.8275     0.5885     0.8885         28        352: 100%|██████████| 22/22 [00:47<00:00,  2.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "                   all          9         17      0.922      0.944      0.954      0.779\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      27/50         0G     0.8101     0.5657     0.8899         24        352: 100%|██████████| 22/22 [00:48<00:00,  2.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
      "                   all          9         17      0.917      0.944      0.942      0.783\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      28/50         0G     0.8112     0.5481     0.8994         32        352: 100%|██████████| 22/22 [00:47<00:00,  2.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "                   all          9         17      0.992      0.923      0.966      0.778\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      29/50         0G     0.8436     0.5664     0.9007         25        352: 100%|██████████| 22/22 [00:47<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "                   all          9         17      0.915      0.944      0.964      0.794\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      30/50         0G     0.7953      0.525     0.8804         27        352: 100%|██████████| 22/22 [00:47<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "                   all          9         17      0.926      0.944      0.971      0.766\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      31/50         0G     0.8058     0.5443     0.8895         22        352: 100%|██████████| 22/22 [00:47<00:00,  2.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.47it/s]\n",
      "                   all          9         17      0.923      0.944      0.961      0.785\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      32/50         0G     0.8127     0.5552      0.892         28        352: 100%|██████████| 22/22 [01:04<00:00,  2.93s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "                   all          9         17      0.934      0.944      0.966       0.76\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      33/50         0G      0.796     0.5219     0.8977         28        352: 100%|██████████| 22/22 [00:47<00:00,  2.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.45it/s]\n",
      "                   all          9         17      0.927      0.944      0.969      0.772\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      34/50         0G     0.8158     0.5307     0.8932         27        352: 100%|██████████| 22/22 [00:47<00:00,  2.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.16it/s]\n",
      "                   all          9         17       0.92      0.944      0.962      0.765\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      35/50         0G     0.7832     0.5218     0.8851         31        352: 100%|██████████| 22/22 [00:47<00:00,  2.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.45it/s]\n",
      "                   all          9         17      0.987       0.93      0.969      0.755\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      36/50         0G     0.7717     0.5381     0.8812         23        352: 100%|██████████| 22/22 [00:47<00:00,  2.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "                   all          9         17      0.978      0.944      0.975      0.771\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      37/50         0G     0.7891      0.516     0.8828         32        352: 100%|██████████| 22/22 [00:47<00:00,  2.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
      "                   all          9         17      0.926      0.944      0.969      0.777\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      38/50         0G     0.7693     0.5133     0.8781         25        352: 100%|██████████| 22/22 [00:48<00:00,  2.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
      "                   all          9         17      0.929      0.944      0.969      0.781\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      39/50         0G     0.7539     0.4981     0.8813         30        352: 100%|██████████| 22/22 [00:50<00:00,  2.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      "                   all          9         17      0.922      0.944      0.969      0.769\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      40/50         0G     0.7471     0.4804     0.8778         26        352: 100%|██████████| 22/22 [00:48<00:00,  2.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "                   all          9         17       0.93      0.944      0.964        0.8\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      41/50         0G     0.7663     0.5279     0.8747         14        352: 100%|██████████| 22/22 [00:47<00:00,  2.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n",
      "                   all          9         17      0.956      0.944      0.981      0.833\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      42/50         0G     0.7455     0.5145     0.8694          9        352: 100%|██████████| 22/22 [00:47<00:00,  2.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\n",
      "                   all          9         17      0.919      0.944      0.969      0.797\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      43/50         0G     0.7443      0.483     0.8673         14        352: 100%|██████████| 22/22 [00:48<00:00,  2.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\n",
      "                   all          9         17      0.922      0.944      0.961      0.811\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      44/50         0G     0.7444     0.4918     0.8669         16        352: 100%|██████████| 22/22 [00:47<00:00,  2.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n",
      "                   all          9         17       0.92      0.944      0.969      0.805\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      45/50         0G     0.7425      0.478     0.8724         15        352: 100%|██████████| 22/22 [00:47<00:00,  2.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.49it/s]\n",
      "                   all          9         17      0.917      0.944      0.966      0.812\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      46/50         0G     0.7393      0.486     0.8738         15        352: 100%|██████████| 22/22 [00:47<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "                   all          9         17      0.916      0.944      0.969      0.809\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      47/50         0G     0.7227     0.4545     0.8642         12        352: 100%|██████████| 22/22 [00:48<00:00,  2.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "                   all          9         17       0.92      0.944      0.961       0.82\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      48/50         0G     0.7255     0.4646     0.8713         15        352: 100%|██████████| 22/22 [00:54<00:00,  2.48s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      "                   all          9         17      0.925      0.944      0.975      0.836\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      49/50         0G     0.7208      0.463     0.8723         13        352: 100%|██████████| 22/22 [00:47<00:00,  2.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
      "                   all          9         17      0.925      0.944      0.978       0.83\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      50/50         0G     0.7099     0.4525     0.8619         14        352: 100%|██████████| 22/22 [00:46<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      "                   all          9         17      0.921      0.944      0.971       0.81\n",
      "\n",
      "50 epochs completed in 0.732 hours.\n",
      "Optimizer stripped from runs\\detect\\train5\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train5\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train5\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.145  Python-3.7.3 torch-1.13.1+cpu CPU (12th Gen Intel Core(TM) i7-1255U)\n",
      "Model summary (fused): 168 layers, 3006038 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.72it/s]\n",
      "                   all          9         17      0.925      0.944      0.975      0.836\n",
      "                   PIC          9          9      0.966      0.889      0.956      0.909\n",
      "       Satyamav Jayate          9          8      0.884          1      0.995      0.763\n",
      "Speed: 0.9ms preprocess, 35.6ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model2.train(\n",
    "    data='data2.yaml',\n",
    "    epochs=50,\n",
    "    batch= 16,\n",
    "    imgsz= 346)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model3 = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.54 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.145  Python-3.7.3 torch-1.13.1+cpu CPU (12th Gen Intel Core(TM) i7-1255U)\n",
      "WARNING  Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data3.yaml, epochs=50, patience=50, batch=16, imgsz=346, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train7\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011628 parameters, 3011612 gradients\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "WARNING  imgsz=[346] must be multiple of max stride 32, updating to [352]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\KIIT0001\\ocr\\ocr_id\\samples\\new_preprocessed\\train2\\labels.cache... 306 images, 38 backgrounds, 0 corrupt: 100%|██████████| 344/344 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\KIIT0001\\ocr\\ocr_id\\samples\\new_preprocessed\\valid2\\labels.cache... 8 images, 1 backgrounds, 0 corrupt: 100%|██████████| 9/9 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train7\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 352 train, 352 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train7\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/50         0G      3.015      4.326      1.764         70        352: 100%|██████████| 22/22 [01:06<00:00,  3.03s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "                   all          9         32    0.00275      0.156    0.00293    0.00153\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/50         0G      2.004      2.736      1.237         45        352: 100%|██████████| 22/22 [01:16<00:00,  3.48s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "                   all          9         32      0.465      0.406      0.315      0.142\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/50         0G      1.867      2.016      1.189         50        352: 100%|██████████| 22/22 [00:58<00:00,  2.66s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "                   all          9         32       0.48      0.232      0.319      0.114\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/50         0G      1.841      1.849      1.167         62        352: 100%|██████████| 22/22 [00:57<00:00,  2.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n",
      "                   all          9         32      0.457        0.5      0.521      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/50         0G      1.726      1.673      1.118         50        352: 100%|██████████| 22/22 [01:16<00:00,  3.47s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "                   all          9         32      0.546      0.717      0.702      0.341\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/50         0G      1.735      1.671      1.118         43        352: 100%|██████████| 22/22 [01:09<00:00,  3.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "                   all          9         32      0.695      0.562      0.707      0.327\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/50         0G       1.69       1.55       1.09         42        352: 100%|██████████| 22/22 [00:57<00:00,  2.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "                   all          9         32       0.75      0.731      0.809      0.424\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/50         0G      1.706      1.534      1.085         57        352: 100%|██████████| 22/22 [00:58<00:00,  2.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "                   all          9         32      0.708      0.778      0.852      0.457\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/50         0G      1.605      1.414      1.061         42        352: 100%|██████████| 22/22 [00:58<00:00,  2.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n",
      "                   all          9         32      0.607      0.905      0.814       0.46\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/50         0G      1.615      1.399      1.054         48        352: 100%|██████████| 22/22 [10:12<00:00, 27.83s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      "                   all          9         32      0.751      0.776       0.87      0.435\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/50         0G      1.568      1.366      1.055         45        352: 100%|██████████| 22/22 [01:09<00:00,  3.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n",
      "                   all          9         32       0.66      0.833      0.809      0.419\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/50         0G      1.565      1.343      1.034         55        352: 100%|██████████| 22/22 [00:58<00:00,  2.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n",
      "                   all          9         32        0.9        0.8      0.902      0.524\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/50         0G      1.582      1.318      1.035         64        352: 100%|██████████| 22/22 [00:58<00:00,  2.66s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "                   all          9         32       0.76      0.918      0.924      0.468\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/50         0G      1.568      1.219      1.019         59        352: 100%|██████████| 22/22 [01:19<00:00,  3.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n",
      "                   all          9         32      0.868      0.822      0.911      0.464\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/50         0G      1.519      1.236      1.029         51        352: 100%|██████████| 22/22 [01:10<00:00,  3.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n",
      "                   all          9         32      0.831      0.795      0.898      0.507\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/50         0G      1.553      1.225      1.023         24        352: 100%|██████████| 22/22 [01:07<00:00,  3.07s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "                   all          9         32      0.825      0.875      0.929      0.459\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/50         0G      1.548      1.206      1.005         55        352: 100%|██████████| 22/22 [01:01<00:00,  2.77s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "                   all          9         32      0.922       0.86      0.907      0.514\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/50         0G      1.516      1.198      1.008         44        352: 100%|██████████| 22/22 [00:58<00:00,  2.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "                   all          9         32      0.778      0.875      0.919      0.497\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/50         0G      1.525      1.186     0.9949         56        352: 100%|██████████| 22/22 [01:18<00:00,  3.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
      "                   all          9         32      0.864      0.805      0.926      0.524\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/50         0G      1.476       1.14     0.9814         34        352: 100%|██████████| 22/22 [01:14<00:00,  3.38s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      "                   all          9         32      0.908      0.841       0.93       0.52\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/50         0G      1.427      1.119     0.9861         44        352: 100%|██████████| 22/22 [01:18<00:00,  3.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "                   all          9         32      0.711      0.848      0.892      0.496\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/50         0G      1.471      1.151     0.9892         68        352: 100%|██████████| 22/22 [00:58<00:00,  2.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n",
      "                   all          9         32      0.667      0.812      0.881      0.487\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/50         0G      1.461      1.158      1.001         18        352: 100%|██████████| 22/22 [00:57<00:00,  2.61s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "                   all          9         32      0.777      0.769      0.896      0.474\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/50         0G      1.534      1.158     0.9884         43        352: 100%|██████████| 22/22 [00:58<00:00,  2.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "                   all          9         32      0.815      0.763      0.887      0.513\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/50         0G      1.436      1.096     0.9827         33        352: 100%|██████████| 22/22 [00:59<00:00,  2.71s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
      "                   all          9         32      0.903      0.821      0.919      0.505\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      26/50         0G      1.414      1.084     0.9894         69        352: 100%|██████████| 22/22 [00:58<00:00,  2.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n",
      "                   all          9         32      0.829      0.799      0.862      0.414\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      27/50         0G      1.455       1.09     0.9757         49        352: 100%|██████████| 22/22 [01:18<00:00,  3.57s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "                   all          9         32      0.859        0.9      0.941       0.51\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      28/50         0G      1.409      1.095     0.9771         59        352: 100%|██████████| 22/22 [01:11<00:00,  3.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n",
      "                   all          9         32      0.921      0.866      0.938      0.539\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      29/50         0G      1.429      1.101     0.9865         33        352: 100%|██████████| 22/22 [00:58<00:00,  2.66s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n",
      "                   all          9         32      0.897       0.85       0.93      0.528\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      30/50         0G      1.391      1.062     0.9674         63        352: 100%|██████████| 22/22 [00:58<00:00,  2.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "                   all          9         32      0.874      0.806       0.91      0.489\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      31/50         0G      1.397      1.051     0.9637         48        352: 100%|██████████| 22/22 [00:58<00:00,  2.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n",
      "                   all          9         32       0.88      0.864      0.919      0.504\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      32/50         0G      1.381      1.058     0.9598         41        352: 100%|██████████| 22/22 [01:10<00:00,  3.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n",
      "                   all          9         32       0.82      0.828      0.924      0.514\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      33/50         0G      1.406      1.036     0.9691         48        352: 100%|██████████| 22/22 [01:25<00:00,  3.89s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
      "                   all          9         32      0.815      0.812      0.894      0.481\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      34/50         0G      1.379      1.044      0.966         44        352: 100%|██████████| 22/22 [01:06<00:00,  3.00s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "                   all          9         32      0.795      0.893      0.926      0.515\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      35/50         0G      1.357      1.025     0.9478         59        352: 100%|██████████| 22/22 [01:14<00:00,  3.40s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "                   all          9         32      0.942      0.887      0.935      0.547\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      36/50         0G      1.336      1.016     0.9652         50        352: 100%|██████████| 22/22 [01:01<00:00,  2.78s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "                   all          9         32      0.922      0.857      0.917      0.552\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      37/50         0G      1.319     0.9914     0.9479         55        352: 100%|██████████| 22/22 [00:58<00:00,  2.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "                   all          9         32      0.939      0.856      0.919      0.526\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      38/50         0G      1.366      1.003      0.954         56        352: 100%|██████████| 22/22 [01:08<00:00,  3.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      "                   all          9         32      0.781      0.903      0.935      0.585\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      39/50         0G      1.329     0.9722     0.9579         44        352: 100%|██████████| 22/22 [01:00<00:00,  2.77s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n",
      "                   all          9         32      0.936      0.851      0.927      0.568\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      40/50         0G       1.29     0.9714      0.945         45        352: 100%|██████████| 22/22 [01:00<00:00,  2.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n",
      "                   all          9         32      0.924      0.868       0.93       0.56\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      41/50         0G      1.352       1.05      0.977         25        352: 100%|██████████| 22/22 [01:18<00:00,  3.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
      "                   all          9         32      0.913      0.899      0.952      0.557\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      42/50         0G      1.329      1.013     0.9748         18        352: 100%|██████████| 22/22 [00:58<00:00,  2.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "                   all          9         32      0.884      0.902      0.932      0.553\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      43/50         0G      1.306      1.009     0.9635         28        352: 100%|██████████| 22/22 [00:57<00:00,  2.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "                   all          9         32      0.863      0.884      0.932      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      44/50         0G      1.309     0.9792     0.9688         24        352: 100%|██████████| 22/22 [00:58<00:00,  2.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n",
      "                   all          9         32      0.853      0.844      0.921      0.558\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      45/50         0G      1.285     0.9655     0.9753         28        352: 100%|██████████| 22/22 [00:57<00:00,  2.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "                   all          9         32      0.883      0.853      0.931      0.564\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      46/50         0G      1.295     0.9639     0.9694         22        352: 100%|██████████| 22/22 [00:57<00:00,  2.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "                   all          9         32      0.872      0.829      0.937      0.554\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      47/50         0G      1.285      0.963     0.9649         23        352: 100%|██████████| 22/22 [00:58<00:00,  2.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "                   all          9         32      0.814      0.885      0.927      0.545\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      48/50         0G      1.289     0.9502     0.9606         24        352: 100%|██████████| 22/22 [00:57<00:00,  2.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.87it/s]\n",
      "                   all          9         32      0.827      0.917      0.939      0.569\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      49/50         0G      1.269     0.9348     0.9701         28        352: 100%|██████████| 22/22 [00:58<00:00,  2.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n",
      "                   all          9         32      0.854      0.887      0.932      0.587\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      50/50         0G      1.254       0.91     0.9645         29        352: 100%|██████████| 22/22 [00:58<00:00,  2.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n",
      "                   all          9         32      0.949      0.833      0.933      0.591\n",
      "\n",
      "50 epochs completed in 1.063 hours.\n",
      "Optimizer stripped from runs\\detect\\train7\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train7\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train7\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.145  Python-3.7.3 torch-1.13.1+cpu CPU (12th Gen Intel Core(TM) i7-1255U)\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "                   all          9         32      0.951      0.832      0.933      0.591\n",
      "                   GOV          9          8      0.884          1      0.967      0.602\n",
      "                  Name          9          8      0.992          1      0.995      0.607\n",
      "                   DOB          9          8      0.927      0.625      0.802       0.55\n",
      "                Gender          9          8          1      0.704      0.967      0.604\n",
      "Speed: 0.9ms preprocess, 44.9ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model3.train(\n",
    "    data='data3.yaml',\n",
    "    epochs=50,\n",
    "    batch= 16,\n",
    "    imgsz= 346)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblib\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: joblib\n",
      "Successfully installed joblib-1.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "torch.save(model2.model.state_dict(), 'yolo_model_weights.pth')  # Save the weights\n",
    "torch.save(model3.model.state_dict(), 'yolo_model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KIIT0001\\ocr\\ocrd\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model4 = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.43  Python-3.7.3 torch-1.13.1+cpu CPU\n",
      "\u001b[34m\u001b[1myolo\\engine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data4.yaml, epochs=50, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, min_memory=False, overlap_mask=True, mask_ratio=4, dropout=False, val=True, split=val, save_json=False, save_hybrid=False, conf=0.001, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=ultralytics/assets/, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs\\detect\\train13\n",
      "Overriding model.yaml nc=1 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.Detect                [2, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011238 parameters, 3011222 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.001), 63 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\KIIT0001\\ocr\\ocr_id\\samples\\new_preprocessed\\train3\\labels.cache... 300 images, 0 backgrounds, 0 corrupt: 100%|██████████| 300/300 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\KIIT0001\\ocr\\ocr_id\\samples\\new_preprocessed\\valid3\\labels.cache... 23 images, 0 backgrounds, 0 corrupt: 100%|██████████| 23/23 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train13\\labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train13\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/50         0G     0.7985      2.842      1.365         33        640: 100%|██████████| 19/19 [02:07<00:00,  6.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.67s/it]\n",
      "                   all         23         23    0.00374          1      0.162     0.0976\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/50         0G     0.5162       2.29       1.17         37        640: 100%|██████████| 19/19 [01:56<00:00,  6.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.59s/it]\n",
      "                   all         23         23    0.00335          1      0.489      0.393\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/50         0G     0.5096       1.63      1.121         38        640: 100%|██████████| 19/19 [01:55<00:00,  6.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.38s/it]\n",
      "                   all         23         23      0.124      0.447      0.384      0.304\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/50         0G     0.5101      1.217      1.111         34        640: 100%|██████████| 19/19 [02:01<00:00,  6.40s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.29s/it]\n",
      "                   all         23         23      0.233      0.371      0.378      0.298\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/50         0G     0.5386      1.131      1.118         36        640: 100%|██████████| 19/19 [01:55<00:00,  6.07s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.21s/it]\n",
      "                   all         23         23      0.239      0.576      0.238      0.143\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/50         0G     0.5541       1.06      1.135         29        640: 100%|██████████| 19/19 [02:08<00:00,  6.76s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.38s/it]\n",
      "                   all         23         23       0.24       0.58      0.344      0.252\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/50         0G     0.5503      1.004       1.11         34        640: 100%|██████████| 19/19 [01:55<00:00,  6.08s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.43s/it]\n",
      "                   all         23         23      0.212      0.707       0.23      0.123\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/50         0G     0.5473     0.9077      1.111         31        640: 100%|██████████| 19/19 [02:02<00:00,  6.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.63s/it]\n",
      "                   all         23         23     0.0872      0.415      0.116     0.0582\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/50         0G     0.5268     0.8591        1.1         43        640: 100%|██████████| 19/19 [02:06<00:00,  6.66s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.54s/it]\n",
      "                   all         23         23      0.121      0.188      0.113     0.0695\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/50         0G     0.5251     0.8335      1.112         32        640: 100%|██████████| 19/19 [02:11<00:00,  6.91s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.68s/it]\n",
      "                   all         23         23      0.129      0.156      0.107     0.0789\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/50         0G     0.5307     0.8157      1.099         38        640: 100%|██████████| 19/19 [03:28<00:00, 10.98s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.84s/it]\n",
      "                   all         23         23      0.158      0.353       0.17     0.0989\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/50         0G     0.5578     0.7824      1.143         39        640: 100%|██████████| 19/19 [02:07<00:00,  6.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.59s/it]\n",
      "                   all         23         23      0.211      0.367      0.209      0.154\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/50         0G     0.4996     0.7416      1.083         33        640: 100%|██████████| 19/19 [02:18<00:00,  7.27s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.21s/it]\n",
      "                   all         23         23      0.201       0.25      0.184      0.132\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/50         0G     0.4867     0.6843       1.07         33        640: 100%|██████████| 19/19 [02:19<00:00,  7.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.69s/it]\n",
      "                   all         23         23      0.144      0.219      0.191      0.152\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/50         0G     0.5053      0.702      1.083         39        640: 100%|██████████| 19/19 [02:08<00:00,  6.75s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.48s/it]\n",
      "                   all         23         23      0.301      0.344      0.299      0.256\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/50         0G     0.4873     0.6547      1.055         35        640: 100%|██████████| 19/19 [02:07<00:00,  6.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.56s/it]\n",
      "                   all         23         23      0.189      0.281      0.238      0.194\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/50         0G     0.4674     0.6017      1.063         39        640: 100%|██████████| 19/19 [02:11<00:00,  6.91s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.90s/it]\n",
      "                   all         23         23       0.13       0.18      0.193      0.163\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/50         0G     0.4541     0.5842      1.049         35        640: 100%|██████████| 19/19 [02:09<00:00,  6.81s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.50s/it]\n",
      "                   all         23         23      0.218      0.312      0.205      0.162\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/50         0G     0.4517     0.5698      1.055         34        640: 100%|██████████| 19/19 [02:07<00:00,  6.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.66s/it]\n",
      "                   all         23         23      0.157      0.156     0.0997     0.0712\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/50         0G     0.4557     0.5834      1.062         37        640: 100%|██████████| 19/19 [02:08<00:00,  6.76s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.59s/it]\n",
      "                   all         23         23     0.0974     0.0938     0.0544     0.0457\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/50         0G     0.4325     0.5281      1.046         35        640: 100%|██████████| 19/19 [02:10<00:00,  6.86s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.67s/it]\n",
      "                   all         23         23     0.0783       0.07     0.0741     0.0653\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/50         0G     0.4474     0.5398      1.048         35        640: 100%|██████████| 19/19 [02:07<00:00,  6.71s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.49s/it]\n",
      "                   all         23         23      0.179      0.308      0.113      0.097\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/50         0G     0.4236     0.5035      1.031         33        640: 100%|██████████| 19/19 [02:13<00:00,  7.02s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.70s/it]\n",
      "                   all         23         23      0.178       0.22      0.136      0.107\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/50         0G     0.4218     0.4963      1.044         37        640: 100%|██████████| 19/19 [02:33<00:00,  8.07s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.49s/it]\n",
      "                   all         23         23      0.135     0.0938     0.0606     0.0526\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/50         0G     0.4123     0.4864      1.021         34        640: 100%|██████████| 19/19 [02:07<00:00,  6.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.53s/it]\n",
      "                   all         23         23      0.135     0.0938     0.0728     0.0614\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      26/50         0G     0.4024     0.4622      1.026         39        640: 100%|██████████| 19/19 [02:07<00:00,  6.69s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.67s/it]\n",
      "                   all         23         23      0.132      0.125       0.14      0.119\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      27/50         0G     0.3959     0.4665       1.01         36        640: 100%|██████████| 19/19 [02:06<00:00,  6.68s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.53s/it]\n",
      "                   all         23         23      0.134      0.156      0.171      0.139\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      28/50         0G     0.4019     0.4524      1.018         40        640: 100%|██████████| 19/19 [02:06<00:00,  6.68s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.49s/it]\n",
      "                   all         23         23      0.118      0.125      0.159      0.137\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      29/50         0G     0.3952     0.4317      1.005         42        640: 100%|██████████| 19/19 [02:07<00:00,  6.69s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.50s/it]\n",
      "                   all         23         23      0.188      0.225      0.209      0.175\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      30/50         0G     0.3857     0.4518      1.033         35        640: 100%|██████████| 19/19 [02:07<00:00,  6.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.73s/it]\n",
      "                   all         23         23      0.199      0.207      0.165      0.127\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      31/50         0G     0.3754      0.438      1.003         34        640: 100%|██████████| 19/19 [02:08<00:00,  6.78s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.50s/it]\n",
      "                   all         23         23       0.21      0.205      0.189      0.135\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      32/50         0G     0.3691     0.3996      1.007         40        640: 100%|██████████| 19/19 [02:07<00:00,  6.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.77s/it]\n",
      "                   all         23         23      0.138      0.119      0.184      0.142\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      33/50         0G     0.3722     0.4079      1.008         42        640: 100%|██████████| 19/19 [02:07<00:00,  6.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.48s/it]\n",
      "                   all         23         23      0.148     0.0938     0.0716     0.0609\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      34/50         0G     0.3702     0.4176      1.007         33        640: 100%|██████████| 19/19 [02:09<00:00,  6.83s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.49s/it]\n",
      "                   all         23         23      0.216      0.214      0.242      0.208\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      35/50         0G      0.347     0.3972     0.9919         39        640: 100%|██████████| 19/19 [02:07<00:00,  6.71s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.54s/it]\n",
      "                   all         23         23      0.148     0.0938     0.0988     0.0824\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      36/50         0G     0.3508     0.3928     0.9972         36        640: 100%|██████████| 19/19 [02:08<00:00,  6.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.47s/it]\n",
      "                   all         23         23      0.147     0.0938     0.0654     0.0559\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      37/50         0G     0.3557     0.3848      1.008         37        640: 100%|██████████| 19/19 [02:06<00:00,  6.68s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.46s/it]\n",
      "                   all         23         23      0.147     0.0938     0.0584     0.0502\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      38/50         0G     0.3406     0.3819      1.003         30        640: 100%|██████████| 19/19 [02:06<00:00,  6.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.47s/it]\n",
      "                   all         23         23      0.148     0.0938     0.0384      0.031\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      39/50         0G     0.3542     0.3817     0.9961         39        640: 100%|██████████| 19/19 [02:06<00:00,  6.66s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.42s/it]\n",
      "                   all         23         23      0.135     0.0938     0.0799     0.0692\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      40/50         0G     0.3349     0.3662     0.9909         33        640: 100%|██████████| 19/19 [02:07<00:00,  6.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.54s/it]\n",
      "                   all         23         23      0.148     0.0938     0.0701      0.059\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      41/50         0G     0.2746     0.4632      1.007         12        640: 100%|██████████| 19/19 [02:00<00:00,  6.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.56s/it]\n",
      "                   all         23         23       0.15     0.0938     0.0655     0.0573\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      42/50         0G     0.2519     0.3959      1.003         13        640: 100%|██████████| 19/19 [02:01<00:00,  6.38s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.49s/it]\n",
      "                   all         23         23      0.179     0.0938     0.0659     0.0584\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      43/50         0G     0.2589     0.3717     0.9963         14        640: 100%|██████████| 19/19 [02:02<00:00,  6.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.54s/it]\n",
      "                   all         23         23      0.149     0.0938     0.0623     0.0523\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      44/50         0G     0.2488     0.3646     0.9934         14        640: 100%|██████████| 19/19 [02:01<00:00,  6.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.44s/it]\n",
      "                   all         23         23      0.149     0.0938     0.0607     0.0526\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      45/50         0G     0.2424     0.3506     0.9757         15        640: 100%|██████████| 19/19 [02:00<00:00,  6.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.43s/it]\n",
      "                   all         23         23      0.149     0.0938     0.0717     0.0631\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      46/50         0G     0.2366      0.325     0.9919         14        640: 100%|██████████| 19/19 [02:01<00:00,  6.38s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.60s/it]\n",
      "                   all         23         23      0.138     0.0938      0.111     0.0918\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      47/50         0G     0.2204     0.3102     0.9704         13        640: 100%|██████████| 19/19 [02:01<00:00,  6.41s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.56s/it]\n",
      "                   all         23         23      0.151      0.109      0.142      0.114\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      48/50         0G     0.2177     0.2966     0.9738         12        640: 100%|██████████| 19/19 [02:14<00:00,  7.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:05<00:00,  5.23s/it]\n",
      "                   all         23         23      0.137     0.0938      0.194      0.155\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      49/50         0G     0.2179     0.2854     0.9715         13        640: 100%|██████████| 19/19 [03:02<00:00,  9.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.76s/it]\n",
      "                   all         23         23      0.178      0.122      0.134      0.105\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      50/50         0G     0.2053     0.3069     0.9613         13        640: 100%|██████████| 19/19 [02:44<00:00,  8.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.86s/it]\n",
      "                   all         23         23      0.149     0.0938     0.0894     0.0717\n",
      "\n",
      "50 epochs completed in 1.869 hours.\n",
      "Optimizer stripped from runs\\detect\\train13\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train13\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train13\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.43  Python-3.7.3 torch-1.13.1+cpu CPU\n",
      "Model summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:05<00:00,  5.27s/it]\n",
      "                   all         23         23    0.00334          1      0.488      0.393\n",
      "              Pan Card         23          7    0.00202          1      0.252      0.184\n",
      "           Aadhar Card         23         16    0.00467          1      0.724      0.602\n",
      "Speed: 2.6ms preprocess, 165.8ms inference, 0.0ms loss, 44.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train13\u001b[0m\n",
      "Results saved to \u001b[1mruns\\detect\\train13\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model4.train(\n",
    "    data='data4.yaml',\n",
    "    epochs=50,\n",
    "    batch= 16,\n",
    "    imgsz= 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model4.model.state_dict(), 'yolo_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the best-performing model\n",
    "model4 = YOLO('runs/detect/train13/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best.pt', 'last.pt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir('runs/detect/train5/weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 Aadhar Cards, 228.4ms\n",
      "Speed: 2.0ms preprocess, 228.4ms inference, 8.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 189.4ms\n",
      "Speed: 2.0ms preprocess, 189.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 176.2ms\n",
      "Speed: 2.0ms preprocess, 176.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 166.7ms\n",
      "Speed: 1.0ms preprocess, 166.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 201.6ms\n",
      "Speed: 1.0ms preprocess, 201.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 166.6ms\n",
      "Speed: 2.0ms preprocess, 166.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 164.1ms\n",
      "Speed: 2.0ms preprocess, 164.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 162.5ms\n",
      "Speed: 1.0ms preprocess, 162.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 162.5ms\n",
      "Speed: 1.0ms preprocess, 162.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 Aadhar Cards, 163.4ms\n",
      "Speed: 1.0ms preprocess, 163.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 Aadhar Cards, 165.5ms\n",
      "Speed: 1.0ms preprocess, 165.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 161.4ms\n",
      "Speed: 1.0ms preprocess, 161.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 Aadhar Cards, 169.8ms\n",
      "Speed: 1.0ms preprocess, 169.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 163.8ms\n",
      "Speed: 1.0ms preprocess, 163.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 165.4ms\n",
      "Speed: 2.0ms preprocess, 165.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 Aadhar Cards, 175.9ms\n",
      "Speed: 1.0ms preprocess, 175.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 Aadhar Cards, 173.3ms\n",
      "Speed: 1.5ms preprocess, 173.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 204.8ms\n",
      "Speed: 7.0ms preprocess, 204.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 Aadhar Cards, 174.3ms\n",
      "Speed: 2.4ms preprocess, 174.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 165.6ms\n",
      "Speed: 2.0ms preprocess, 165.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 161.8ms\n",
      "Speed: 2.0ms preprocess, 161.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 157.4ms\n",
      "Speed: 2.0ms preprocess, 157.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 182.1ms\n",
      "Speed: 2.0ms preprocess, 182.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 160.5ms\n",
      "Speed: 3.0ms preprocess, 160.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 171.8ms\n",
      "Speed: 1.0ms preprocess, 171.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 173.0ms\n",
      "Speed: 2.0ms preprocess, 173.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 180.9ms\n",
      "Speed: 1.0ms preprocess, 180.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 188.8ms\n",
      "Speed: 2.0ms preprocess, 188.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 Aadhar Cards, 188.2ms\n",
      "Speed: 1.5ms preprocess, 188.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 Aadhar Cards, 182.4ms\n",
      "Speed: 1.3ms preprocess, 182.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 Aadhar Cards, 187.5ms\n",
      "Speed: 2.0ms preprocess, 187.5ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 194.7ms\n",
      "Speed: 2.0ms preprocess, 194.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 175.9ms\n",
      "Speed: 2.0ms preprocess, 175.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 215.8ms\n",
      "Speed: 1.0ms preprocess, 215.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 182.2ms\n",
      "Speed: 1.5ms preprocess, 182.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 202.5ms\n",
      "Speed: 1.0ms preprocess, 202.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 214.2ms\n",
      "Speed: 1.0ms preprocess, 214.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 182.1ms\n",
      "Speed: 1.0ms preprocess, 182.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 215.0ms\n",
      "Speed: 2.0ms preprocess, 215.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 191.8ms\n",
      "Speed: 2.5ms preprocess, 191.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 203.3ms\n",
      "Speed: 2.0ms preprocess, 203.3ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 212.6ms\n",
      "Speed: 1.0ms preprocess, 212.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 218.0ms\n",
      "Speed: 1.0ms preprocess, 218.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 212.4ms\n",
      "Speed: 2.0ms preprocess, 212.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 185.7ms\n",
      "Speed: 1.4ms preprocess, 185.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 221.8ms\n",
      "Speed: 1.0ms preprocess, 221.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 187.6ms\n",
      "Speed: 1.0ms preprocess, 187.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 190.9ms\n",
      "Speed: 1.0ms preprocess, 190.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 193.3ms\n",
      "Speed: 1.0ms preprocess, 193.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 207.4ms\n",
      "Speed: 2.0ms preprocess, 207.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 178.3ms\n",
      "Speed: 1.0ms preprocess, 178.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 184.5ms\n",
      "Speed: 1.0ms preprocess, 184.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 215.3ms\n",
      "Speed: 2.0ms preprocess, 215.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 212.6ms\n",
      "Speed: 2.0ms preprocess, 212.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 165.6ms\n",
      "Speed: 0.9ms preprocess, 165.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 183.4ms\n",
      "Speed: 2.0ms preprocess, 183.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 179.4ms\n",
      "Speed: 2.0ms preprocess, 179.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 175.0ms\n",
      "Speed: 2.0ms preprocess, 175.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 187.2ms\n",
      "Speed: 2.0ms preprocess, 187.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 183.5ms\n",
      "Speed: 1.0ms preprocess, 183.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 199.5ms\n",
      "Speed: 2.0ms preprocess, 199.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 185.7ms\n",
      "Speed: 12.0ms preprocess, 185.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 216.6ms\n",
      "Speed: 1.0ms preprocess, 216.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 212.0ms\n",
      "Speed: 1.0ms preprocess, 212.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 195.1ms\n",
      "Speed: 2.0ms preprocess, 195.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 200.4ms\n",
      "Speed: 1.0ms preprocess, 200.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 208.5ms\n",
      "Speed: 2.0ms preprocess, 208.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 177.6ms\n",
      "Speed: 3.1ms preprocess, 177.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 203.4ms\n",
      "Speed: 1.0ms preprocess, 203.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 168.3ms\n",
      "Speed: 1.0ms preprocess, 168.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 173.5ms\n",
      "Speed: 2.0ms preprocess, 173.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 183.9ms\n",
      "Speed: 1.0ms preprocess, 183.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 173.5ms\n",
      "Speed: 1.0ms preprocess, 173.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 173.1ms\n",
      "Speed: 1.0ms preprocess, 173.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 183.2ms\n",
      "Speed: 1.0ms preprocess, 183.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 203.9ms\n",
      "Speed: 2.0ms preprocess, 203.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 190.2ms\n",
      "Speed: 2.0ms preprocess, 190.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 191.6ms\n",
      "Speed: 1.0ms preprocess, 191.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 192.6ms\n",
      "Speed: 1.9ms preprocess, 192.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 201.0ms\n",
      "Speed: 2.0ms preprocess, 201.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 196.4ms\n",
      "Speed: 1.0ms preprocess, 196.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 230.8ms\n",
      "Speed: 2.0ms preprocess, 230.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 187.6ms\n",
      "Speed: 1.0ms preprocess, 187.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 205.3ms\n",
      "Speed: 2.0ms preprocess, 205.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 208.8ms\n",
      "Speed: 1.0ms preprocess, 208.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 198.6ms\n",
      "Speed: 1.0ms preprocess, 198.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 196.2ms\n",
      "Speed: 2.0ms preprocess, 196.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 179.3ms\n",
      "Speed: 1.0ms preprocess, 179.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 192.2ms\n",
      "Speed: 1.0ms preprocess, 192.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 193.8ms\n",
      "Speed: 2.0ms preprocess, 193.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 210.8ms\n",
      "Speed: 2.0ms preprocess, 210.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 180.1ms\n",
      "Speed: 1.2ms preprocess, 180.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 190.8ms\n",
      "Speed: 2.0ms preprocess, 190.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 186.6ms\n",
      "Speed: 1.5ms preprocess, 186.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 178.2ms\n",
      "Speed: 2.0ms preprocess, 178.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 200.0ms\n",
      "Speed: 2.0ms preprocess, 200.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 218.7ms\n",
      "Speed: 2.0ms preprocess, 218.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 177.6ms\n",
      "Speed: 1.0ms preprocess, 177.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 179.1ms\n",
      "Speed: 2.0ms preprocess, 179.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 194.6ms\n",
      "Speed: 2.0ms preprocess, 194.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 181.5ms\n",
      "Speed: 1.3ms preprocess, 181.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 171.8ms\n",
      "Speed: 2.0ms preprocess, 171.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 177.2ms\n",
      "Speed: 1.0ms preprocess, 177.2ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 193.2ms\n",
      "Speed: 1.0ms preprocess, 193.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 194.9ms\n",
      "Speed: 1.0ms preprocess, 194.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 188.8ms\n",
      "Speed: 2.0ms preprocess, 188.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 190.7ms\n",
      "Speed: 2.0ms preprocess, 190.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 213.1ms\n",
      "Speed: 1.6ms preprocess, 213.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 201.6ms\n",
      "Speed: 2.0ms preprocess, 201.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 186.5ms\n",
      "Speed: 1.0ms preprocess, 186.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 200.7ms\n",
      "Speed: 1.0ms preprocess, 200.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 196.6ms\n",
      "Speed: 2.4ms preprocess, 196.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 208.4ms\n",
      "Speed: 2.0ms preprocess, 208.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 215.1ms\n",
      "Speed: 1.0ms preprocess, 215.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 205.7ms\n",
      "Speed: 2.0ms preprocess, 205.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 192.2ms\n",
      "Speed: 1.0ms preprocess, 192.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 178.1ms\n",
      "Speed: 1.0ms preprocess, 178.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 181.4ms\n",
      "Speed: 1.0ms preprocess, 181.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 163.5ms\n",
      "Speed: 1.0ms preprocess, 163.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 166.6ms\n",
      "Speed: 2.1ms preprocess, 166.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 169.2ms\n",
      "Speed: 2.0ms preprocess, 169.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 171.9ms\n",
      "Speed: 1.0ms preprocess, 171.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 160.9ms\n",
      "Speed: 1.0ms preprocess, 160.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 173.5ms\n",
      "Speed: 1.0ms preprocess, 173.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 161.6ms\n",
      "Speed: 1.4ms preprocess, 161.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 163.1ms\n",
      "Speed: 1.0ms preprocess, 163.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 165.5ms\n",
      "Speed: 1.0ms preprocess, 165.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 161.4ms\n",
      "Speed: 1.0ms preprocess, 161.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 166.0ms\n",
      "Speed: 2.0ms preprocess, 166.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 195.5ms\n",
      "Speed: 1.0ms preprocess, 195.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 182.2ms\n",
      "Speed: 1.0ms preprocess, 182.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 161.6ms\n",
      "Speed: 1.0ms preprocess, 161.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 163.0ms\n",
      "Speed: 1.0ms preprocess, 163.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 161.8ms\n",
      "Speed: 2.0ms preprocess, 161.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 168.3ms\n",
      "Speed: 1.0ms preprocess, 168.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 163.9ms\n",
      "Speed: 1.0ms preprocess, 163.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 155.0ms\n",
      "Speed: 1.0ms preprocess, 155.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 156.0ms\n",
      "Speed: 2.0ms preprocess, 156.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 161.9ms\n",
      "Speed: 1.0ms preprocess, 161.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 162.9ms\n",
      "Speed: 0.0ms preprocess, 162.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 174.7ms\n",
      "Speed: 1.0ms preprocess, 174.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 161.8ms\n",
      "Speed: 1.0ms preprocess, 161.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 168.3ms\n",
      "Speed: 1.0ms preprocess, 168.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 164.5ms\n",
      "Speed: 1.0ms preprocess, 164.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 182.8ms\n",
      "Speed: 1.0ms preprocess, 182.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 175.4ms\n",
      "Speed: 1.0ms preprocess, 175.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 176.1ms\n",
      "Speed: 1.0ms preprocess, 176.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 185.5ms\n",
      "Speed: 1.0ms preprocess, 185.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 179.8ms\n",
      "Speed: 2.0ms preprocess, 179.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 184.5ms\n",
      "Speed: 2.0ms preprocess, 184.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 174.7ms\n",
      "Speed: 1.2ms preprocess, 174.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 183.2ms\n",
      "Speed: 2.0ms preprocess, 183.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 176.0ms\n",
      "Speed: 1.0ms preprocess, 176.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 164.9ms\n",
      "Speed: 1.0ms preprocess, 164.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 161.2ms\n",
      "Speed: 1.0ms preprocess, 161.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 161.5ms\n",
      "Speed: 1.0ms preprocess, 161.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 175.5ms\n",
      "Speed: 1.0ms preprocess, 175.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 187.5ms\n",
      "Speed: 1.0ms preprocess, 187.5ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 172.5ms\n",
      "Speed: 1.0ms preprocess, 172.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 162.7ms\n",
      "Speed: 1.0ms preprocess, 162.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 160.7ms\n",
      "Speed: 1.0ms preprocess, 160.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 159.6ms\n",
      "Speed: 1.0ms preprocess, 159.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 165.7ms\n",
      "Speed: 1.0ms preprocess, 165.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 162.5ms\n",
      "Speed: 0.0ms preprocess, 162.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 165.8ms\n",
      "Speed: 1.0ms preprocess, 165.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 164.8ms\n",
      "Speed: 1.1ms preprocess, 164.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 203.6ms\n",
      "Speed: 2.3ms preprocess, 203.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 171.1ms\n",
      "Speed: 2.0ms preprocess, 171.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 179.4ms\n",
      "Speed: 2.0ms preprocess, 179.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 165.9ms\n",
      "Speed: 1.0ms preprocess, 165.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 177.2ms\n",
      "Speed: 2.0ms preprocess, 177.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 165.8ms\n",
      "Speed: 2.0ms preprocess, 165.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 173.6ms\n",
      "Speed: 1.5ms preprocess, 173.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 193.9ms\n",
      "Speed: 2.0ms preprocess, 193.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 174.7ms\n",
      "Speed: 1.0ms preprocess, 174.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 183.5ms\n",
      "Speed: 1.0ms preprocess, 183.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 182.2ms\n",
      "Speed: 1.0ms preprocess, 182.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 167.1ms\n",
      "Speed: 1.0ms preprocess, 167.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 177.9ms\n",
      "Speed: 1.0ms preprocess, 177.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 185.7ms\n",
      "Speed: 1.0ms preprocess, 185.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 170.7ms\n",
      "Speed: 1.0ms preprocess, 170.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 170.5ms\n",
      "Speed: 1.0ms preprocess, 170.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 168.0ms\n",
      "Speed: 1.0ms preprocess, 168.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 188.4ms\n",
      "Speed: 2.0ms preprocess, 188.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 198.2ms\n",
      "Speed: 2.0ms preprocess, 198.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 170.5ms\n",
      "Speed: 1.0ms preprocess, 170.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 170.5ms\n",
      "Speed: 1.0ms preprocess, 170.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 165.4ms\n",
      "Speed: 1.0ms preprocess, 165.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 177.9ms\n",
      "Speed: 1.0ms preprocess, 177.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 188.9ms\n",
      "Speed: 1.0ms preprocess, 188.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 171.5ms\n",
      "Speed: 2.0ms preprocess, 171.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 172.4ms\n",
      "Speed: 1.0ms preprocess, 172.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 170.5ms\n",
      "Speed: 2.0ms preprocess, 170.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 170.3ms\n",
      "Speed: 1.0ms preprocess, 170.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 167.5ms\n",
      "Speed: 1.0ms preprocess, 167.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 168.4ms\n",
      "Speed: 1.0ms preprocess, 168.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 188.1ms\n",
      "Speed: 1.0ms preprocess, 188.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 175.0ms\n",
      "Speed: 1.0ms preprocess, 175.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 179.4ms\n",
      "Speed: 2.0ms preprocess, 179.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 176.0ms\n",
      "Speed: 1.0ms preprocess, 176.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 162.7ms\n",
      "Speed: 1.0ms preprocess, 162.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 174.6ms\n",
      "Speed: 1.0ms preprocess, 174.6ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 200.3ms\n",
      "Speed: 1.0ms preprocess, 200.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 162.6ms\n",
      "Speed: 1.0ms preprocess, 162.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 166.1ms\n",
      "Speed: 1.0ms preprocess, 166.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 169.8ms\n",
      "Speed: 2.0ms preprocess, 169.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 171.8ms\n",
      "Speed: 1.5ms preprocess, 171.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 189.6ms\n",
      "Speed: 1.0ms preprocess, 189.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 188.5ms\n",
      "Speed: 2.0ms preprocess, 188.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 169.9ms\n",
      "Speed: 1.0ms preprocess, 169.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 164.7ms\n",
      "Speed: 1.0ms preprocess, 164.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 179.9ms\n",
      "Speed: 2.0ms preprocess, 179.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 183.8ms\n",
      "Speed: 1.0ms preprocess, 183.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 187.5ms\n",
      "Speed: 1.0ms preprocess, 187.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 Aadhar Cards, 171.3ms\n",
      "Speed: 2.0ms preprocess, 171.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 196.6ms\n",
      "Speed: 2.0ms preprocess, 196.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 185.6ms\n",
      "Speed: 1.0ms preprocess, 185.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 206.1ms\n",
      "Speed: 2.0ms preprocess, 206.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 205.5ms\n",
      "Speed: 1.0ms preprocess, 205.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 208.6ms\n",
      "Speed: 1.0ms preprocess, 208.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 222.5ms\n",
      "Speed: 2.0ms preprocess, 222.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 218.9ms\n",
      "Speed: 2.0ms preprocess, 218.9ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 223.0ms\n",
      "Speed: 2.0ms preprocess, 223.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 194.0ms\n",
      "Speed: 1.0ms preprocess, 194.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 236.3ms\n",
      "Speed: 2.0ms preprocess, 236.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 Aadhar Cards, 191.9ms\n",
      "Speed: 1.0ms preprocess, 191.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 195.9ms\n",
      "Speed: 1.0ms preprocess, 195.9ms inference, 12.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 208.9ms\n",
      "Speed: 2.0ms preprocess, 208.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 201.4ms\n",
      "Speed: 1.0ms preprocess, 201.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 187.2ms\n",
      "Speed: 2.0ms preprocess, 187.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 193.4ms\n",
      "Speed: 1.0ms preprocess, 193.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 174.2ms\n",
      "Speed: 1.0ms preprocess, 174.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 204.8ms\n",
      "Speed: 1.0ms preprocess, 204.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 189.2ms\n",
      "Speed: 1.0ms preprocess, 189.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 187.9ms\n",
      "Speed: 2.0ms preprocess, 187.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 231.0ms\n",
      "Speed: 1.6ms preprocess, 231.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 182.5ms\n",
      "Speed: 1.0ms preprocess, 182.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 201.1ms\n",
      "Speed: 1.0ms preprocess, 201.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 195.8ms\n",
      "Speed: 2.0ms preprocess, 195.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 197.2ms\n",
      "Speed: 1.0ms preprocess, 197.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 174.2ms\n",
      "Speed: 1.0ms preprocess, 174.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 191.7ms\n",
      "Speed: 1.0ms preprocess, 191.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 189.3ms\n",
      "Speed: 1.0ms preprocess, 189.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 196.6ms\n",
      "Speed: 1.0ms preprocess, 196.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 211.9ms\n",
      "Speed: 1.0ms preprocess, 211.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 180.7ms\n",
      "Speed: 1.0ms preprocess, 180.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 180.7ms\n",
      "Speed: 1.0ms preprocess, 180.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 200.3ms\n",
      "Speed: 2.0ms preprocess, 200.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 182.9ms\n",
      "Speed: 3.0ms preprocess, 182.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 208.1ms\n",
      "Speed: 2.0ms preprocess, 208.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 195.3ms\n",
      "Speed: 2.0ms preprocess, 195.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 204.0ms\n",
      "Speed: 2.0ms preprocess, 204.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 196.0ms\n",
      "Speed: 2.0ms preprocess, 196.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 191.3ms\n",
      "Speed: 1.0ms preprocess, 191.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 193.6ms\n",
      "Speed: 1.0ms preprocess, 193.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 195.3ms\n",
      "Speed: 2.0ms preprocess, 195.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 199.0ms\n",
      "Speed: 1.0ms preprocess, 199.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 193.7ms\n",
      "Speed: 2.0ms preprocess, 193.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 182.7ms\n",
      "Speed: 1.2ms preprocess, 182.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 182.0ms\n",
      "Speed: 1.0ms preprocess, 182.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 209.4ms\n",
      "Speed: 1.0ms preprocess, 209.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 187.8ms\n",
      "Speed: 1.0ms preprocess, 187.8ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 202.9ms\n",
      "Speed: 2.0ms preprocess, 202.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 180.8ms\n",
      "Speed: 1.0ms preprocess, 180.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 173.0ms\n",
      "Speed: 2.0ms preprocess, 173.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 208.1ms\n",
      "Speed: 2.0ms preprocess, 208.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 192.9ms\n",
      "Speed: 2.0ms preprocess, 192.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 190.9ms\n",
      "Speed: 1.0ms preprocess, 190.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 222.6ms\n",
      "Speed: 2.0ms preprocess, 222.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 213.4ms\n",
      "Speed: 2.0ms preprocess, 213.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 209.6ms\n",
      "Speed: 1.0ms preprocess, 209.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 215.3ms\n",
      "Speed: 2.0ms preprocess, 215.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 186.6ms\n",
      "Speed: 1.0ms preprocess, 186.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 185.9ms\n",
      "Speed: 1.0ms preprocess, 185.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 188.6ms\n",
      "Speed: 1.0ms preprocess, 188.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 214.9ms\n",
      "Speed: 2.0ms preprocess, 214.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 3 Aadhar Cards, 178.3ms\n",
      "Speed: 2.0ms preprocess, 178.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 212.4ms\n",
      "Speed: 2.0ms preprocess, 212.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 201.6ms\n",
      "Speed: 1.0ms preprocess, 201.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 4 Aadhar Cards, 189.6ms\n",
      "Speed: 1.3ms preprocess, 189.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 2 Aadhar Cards, 175.6ms\n",
      "Speed: 1.0ms preprocess, 175.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 169.2ms\n",
      "Speed: 2.0ms preprocess, 169.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 166.5ms\n",
      "Speed: 1.0ms preprocess, 166.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Card, 1 Aadhar Card, 173.9ms\n",
      "Speed: 1.0ms preprocess, 173.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 184.4ms\n",
      "Speed: 2.0ms preprocess, 184.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 177.9ms\n",
      "Speed: 2.0ms preprocess, 177.9ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 192.4ms\n",
      "Speed: 1.0ms preprocess, 192.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 166.4ms\n",
      "Speed: 1.0ms preprocess, 166.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 161.2ms\n",
      "Speed: 1.0ms preprocess, 161.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 166.7ms\n",
      "Speed: 1.0ms preprocess, 166.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 162.9ms\n",
      "Speed: 2.0ms preprocess, 162.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 164.2ms\n",
      "Speed: 1.0ms preprocess, 164.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 160.1ms\n",
      "Speed: 1.0ms preprocess, 160.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 170.4ms\n",
      "Speed: 0.0ms preprocess, 170.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 161.0ms\n",
      "Speed: 1.0ms preprocess, 161.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 163.0ms\n",
      "Speed: 1.0ms preprocess, 163.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 159.9ms\n",
      "Speed: 2.1ms preprocess, 159.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 161.7ms\n",
      "Speed: 1.0ms preprocess, 161.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 168.2ms\n",
      "Speed: 1.0ms preprocess, 168.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 161.6ms\n",
      "Speed: 1.0ms preprocess, 161.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 162.8ms\n",
      "Speed: 2.0ms preprocess, 162.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 166.0ms\n",
      "Speed: 1.5ms preprocess, 166.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 197.9ms\n",
      "Speed: 2.0ms preprocess, 197.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 173.3ms\n",
      "Speed: 1.0ms preprocess, 173.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 168.6ms\n",
      "Speed: 2.1ms preprocess, 168.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 168.5ms\n",
      "Speed: 1.0ms preprocess, 168.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 159.0ms\n",
      "Speed: 1.2ms preprocess, 159.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 162.0ms\n",
      "Speed: 1.0ms preprocess, 162.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 162.5ms\n",
      "Speed: 2.0ms preprocess, 162.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 168.1ms\n",
      "Speed: 1.9ms preprocess, 168.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 167.9ms\n",
      "Speed: 2.0ms preprocess, 167.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 156.1ms\n",
      "Speed: 1.0ms preprocess, 156.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 163.2ms\n",
      "Speed: 1.0ms preprocess, 163.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 167.4ms\n",
      "Speed: 1.1ms preprocess, 167.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 226.8ms\n",
      "Speed: 0.0ms preprocess, 226.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 175.6ms\n",
      "Speed: 2.0ms preprocess, 175.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 177.7ms\n",
      "Speed: 1.0ms preprocess, 177.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 162.4ms\n",
      "Speed: 1.0ms preprocess, 162.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 192.1ms\n",
      "Speed: 1.4ms preprocess, 192.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 179.7ms\n",
      "Speed: 2.0ms preprocess, 179.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 186.8ms\n",
      "Speed: 1.0ms preprocess, 186.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 201.2ms\n",
      "Speed: 1.0ms preprocess, 201.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 224.3ms\n",
      "Speed: 1.0ms preprocess, 224.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 181.8ms\n",
      "Speed: 1.0ms preprocess, 181.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 195.3ms\n",
      "Speed: 2.0ms preprocess, 195.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 231.9ms\n",
      "Speed: 2.0ms preprocess, 231.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 186.9ms\n",
      "Speed: 1.0ms preprocess, 186.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 190.0ms\n",
      "Speed: 1.0ms preprocess, 190.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 212.4ms\n",
      "Speed: 2.0ms preprocess, 212.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 197.1ms\n",
      "Speed: 2.0ms preprocess, 197.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 212.4ms\n",
      "Speed: 2.0ms preprocess, 212.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 199.1ms\n",
      "Speed: 1.3ms preprocess, 199.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Aadhar Card, 199.5ms\n",
      "Speed: 3.0ms preprocess, 199.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 206.9ms\n",
      "Speed: 1.0ms preprocess, 206.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 Aadhar Cards, 212.5ms\n",
      "Speed: 2.0ms preprocess, 212.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 176.2ms\n",
      "Speed: 1.0ms preprocess, 176.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 168.1ms\n",
      "Speed: 1.0ms preprocess, 168.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 180.0ms\n",
      "Speed: 1.0ms preprocess, 180.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 191.5ms\n",
      "Speed: 1.0ms preprocess, 191.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 184.8ms\n",
      "Speed: 1.0ms preprocess, 184.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 199.8ms\n",
      "Speed: 1.0ms preprocess, 199.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Aadhar Cards, 201.4ms\n",
      "Speed: 2.0ms preprocess, 201.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([1.]), Confidence: tensor([0.02336]), Coordinates: tensor([[166., 233., 640., 480.]])\n",
      "Class: tensor([1.]), Confidence: tensor([0.02263]), Coordinates: tensor([[  4.,   2., 638., 480.]])\n"
     ]
    }
   ],
   "source": [
    "# Open a webcam feed or video file\n",
    "import cv2\n",
    "import os\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for webcam, or replace with video file path\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run the model on the frame\n",
    "    results = model4.predict(source=frame, conf=0.02)\n",
    "\n",
    "    # Render the results on the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow('YOLO Detection', annotated_frame)\n",
    "\n",
    "    # Break on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "for box in results[0].boxes:\n",
    "    print(f\"Class: {box.cls}, Confidence: {box.conf}, Coordinates: {box.xyxy}\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 288x352 (no detections), 144.3ms\n",
      "Speed: 2.9ms preprocess, 144.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 108.4ms\n",
      "Speed: 1.0ms preprocess, 108.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 126.1ms\n",
      "Speed: 3.9ms preprocess, 126.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 112.5ms\n",
      "Speed: 8.3ms preprocess, 112.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 119.8ms\n",
      "Speed: 2.0ms preprocess, 119.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 140.0ms\n",
      "Speed: 0.0ms preprocess, 140.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 138.0ms\n",
      "Speed: 0.0ms preprocess, 138.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 128.3ms\n",
      "Speed: 2.6ms preprocess, 128.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 138.9ms\n",
      "Speed: 4.1ms preprocess, 138.9ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 127.2ms\n",
      "Speed: 15.2ms preprocess, 127.2ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 129.7ms\n",
      "Speed: 0.0ms preprocess, 129.7ms inference, 5.5ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 133.7ms\n",
      "Speed: 0.0ms preprocess, 133.7ms inference, 7.5ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 118.4ms\n",
      "Speed: 0.0ms preprocess, 118.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 144.5ms\n",
      "Speed: 0.0ms preprocess, 144.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 137.1ms\n",
      "Speed: 4.2ms preprocess, 137.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 121.0ms\n",
      "Speed: 0.0ms preprocess, 121.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 2 PICs, 1 Satyamav Jayate, 107.5ms\n",
      "Speed: 2.8ms preprocess, 107.5ms inference, 8.3ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 115.4ms\n",
      "Speed: 0.0ms preprocess, 115.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 116.5ms\n",
      "Speed: 0.0ms preprocess, 116.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 116.2ms\n",
      "Speed: 11.4ms preprocess, 116.2ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 2 PICs, 120.7ms\n",
      "Speed: 2.8ms preprocess, 120.7ms inference, 9.6ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 108.7ms\n",
      "Speed: 0.0ms preprocess, 108.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 116.0ms\n",
      "Speed: 0.0ms preprocess, 116.0ms inference, 4.6ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 105.1ms\n",
      "Speed: 2.1ms preprocess, 105.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 96.6ms\n",
      "Speed: 3.3ms preprocess, 96.6ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 111.8ms\n",
      "Speed: 0.0ms preprocess, 111.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 116.2ms\n",
      "Speed: 2.7ms preprocess, 116.2ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Gender, 105.5ms\n",
      "Speed: 8.2ms preprocess, 105.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 112.6ms\n",
      "Speed: 1.0ms preprocess, 112.6ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 122.9ms\n",
      "Speed: 0.0ms preprocess, 122.9ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 131.3ms\n",
      "Speed: 1.9ms preprocess, 131.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 118.4ms\n",
      "Speed: 0.0ms preprocess, 118.4ms inference, 13.5ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 109.5ms\n",
      "Speed: 3.7ms preprocess, 109.5ms inference, 8.5ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 101.1ms\n",
      "Speed: 0.0ms preprocess, 101.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 125.7ms\n",
      "Speed: 0.0ms preprocess, 125.7ms inference, 3.3ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 Gender, 114.2ms\n",
      "Speed: 0.0ms preprocess, 114.2ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 110.1ms\n",
      "Speed: 3.2ms preprocess, 110.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 111.8ms\n",
      "Speed: 11.2ms preprocess, 111.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 106.5ms\n",
      "Speed: 0.0ms preprocess, 106.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 122.2ms\n",
      "Speed: 0.0ms preprocess, 122.2ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 113.8ms\n",
      "Speed: 1.6ms preprocess, 113.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Gender, 101.3ms\n",
      "Speed: 2.0ms preprocess, 101.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 2 Satyamav Jayates, 133.9ms\n",
      "Speed: 0.0ms preprocess, 133.9ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Gender, 112.0ms\n",
      "Speed: 6.2ms preprocess, 112.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 107.9ms\n",
      "Speed: 0.0ms preprocess, 107.9ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Gender, 118.2ms\n",
      "Speed: 0.0ms preprocess, 118.2ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 115.6ms\n",
      "Speed: 0.0ms preprocess, 115.6ms inference, 3.6ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 102.8ms\n",
      "Speed: 0.0ms preprocess, 102.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 129.7ms\n",
      "Speed: 0.0ms preprocess, 129.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 112.5ms\n",
      "Speed: 0.0ms preprocess, 112.5ms inference, 2.9ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 2 Satyamav Jayates, 120.2ms\n",
      "Speed: 0.0ms preprocess, 120.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 118.3ms\n",
      "Speed: 0.0ms preprocess, 118.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 107.7ms\n",
      "Speed: 3.2ms preprocess, 107.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 110.7ms\n",
      "Speed: 0.0ms preprocess, 110.7ms inference, 4.2ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 3 Satyamav Jayates, 112.1ms\n",
      "Speed: 3.9ms preprocess, 112.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 Name, 1 DOB, 1 Gender, 105.5ms\n",
      "Speed: 0.0ms preprocess, 105.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 116.9ms\n",
      "Speed: 0.0ms preprocess, 116.9ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Gender, 120.6ms\n",
      "Speed: 0.0ms preprocess, 120.6ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 101.8ms\n",
      "Speed: 0.0ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 116.8ms\n",
      "Speed: 5.1ms preprocess, 116.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 113.5ms\n",
      "Speed: 2.9ms preprocess, 113.5ms inference, 8.2ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 104.3ms\n",
      "Speed: 0.0ms preprocess, 104.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 109.4ms\n",
      "Speed: 0.0ms preprocess, 109.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 97.6ms\n",
      "Speed: 0.0ms preprocess, 97.6ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 105.1ms\n",
      "Speed: 0.0ms preprocess, 105.1ms inference, 10.2ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 112.5ms\n",
      "Speed: 0.0ms preprocess, 112.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 114.8ms\n",
      "Speed: 0.0ms preprocess, 114.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 Gender, 108.9ms\n",
      "Speed: 0.0ms preprocess, 108.9ms inference, 12.9ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 105.6ms\n",
      "Speed: 0.0ms preprocess, 105.6ms inference, 15.8ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 107.4ms\n",
      "Speed: 0.0ms preprocess, 107.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 123.0ms\n",
      "Speed: 2.9ms preprocess, 123.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 114.0ms\n",
      "Speed: 0.0ms preprocess, 114.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 114.3ms\n",
      "Speed: 0.0ms preprocess, 114.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 131.5ms\n",
      "Speed: 0.0ms preprocess, 131.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 110.8ms\n",
      "Speed: 0.0ms preprocess, 110.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 120.8ms\n",
      "Speed: 0.0ms preprocess, 120.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 102.3ms\n",
      "Speed: 1.1ms preprocess, 102.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 100.5ms\n",
      "Speed: 5.4ms preprocess, 100.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 103.7ms\n",
      "Speed: 2.8ms preprocess, 103.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 113.9ms\n",
      "Speed: 0.0ms preprocess, 113.9ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 113.2ms\n",
      "Speed: 3.2ms preprocess, 113.2ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 118.0ms\n",
      "Speed: 0.0ms preprocess, 118.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 119.9ms\n",
      "Speed: 0.0ms preprocess, 119.9ms inference, 3.4ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 109.4ms\n",
      "Speed: 0.0ms preprocess, 109.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 123.2ms\n",
      "Speed: 5.9ms preprocess, 123.2ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 95.1ms\n",
      "Speed: 6.5ms preprocess, 95.1ms inference, 6.8ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 111.1ms\n",
      "Speed: 1.1ms preprocess, 111.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 112.1ms\n",
      "Speed: 0.0ms preprocess, 112.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 99.9ms\n",
      "Speed: 4.0ms preprocess, 99.9ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Gender, 108.2ms\n",
      "Speed: 3.7ms preprocess, 108.2ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 113.2ms\n",
      "Speed: 2.9ms preprocess, 113.2ms inference, 7.8ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 99.7ms\n",
      "Speed: 0.0ms preprocess, 99.7ms inference, 8.5ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 115.3ms\n",
      "Speed: 3.6ms preprocess, 115.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 Gender, 100.8ms\n",
      "Speed: 0.0ms preprocess, 100.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 103.4ms\n",
      "Speed: 0.0ms preprocess, 103.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 108.5ms\n",
      "Speed: 0.0ms preprocess, 108.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 108.6ms\n",
      "Speed: 0.0ms preprocess, 108.6ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 98.0ms\n",
      "Speed: 0.0ms preprocess, 98.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 119.9ms\n",
      "Speed: 2.6ms preprocess, 119.9ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 98.3ms\n",
      "Speed: 0.0ms preprocess, 98.3ms inference, 3.5ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 120.7ms\n",
      "Speed: 0.0ms preprocess, 120.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 113.2ms\n",
      "Speed: 0.0ms preprocess, 113.2ms inference, 5.5ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 108.9ms\n",
      "Speed: 3.2ms preprocess, 108.9ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 117.2ms\n",
      "Speed: 0.0ms preprocess, 117.2ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 110.6ms\n",
      "Speed: 2.6ms preprocess, 110.6ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 111.1ms\n",
      "Speed: 1.5ms preprocess, 111.1ms inference, 2.9ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 125.0ms\n",
      "Speed: 3.0ms preprocess, 125.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 2 Genders, 110.1ms\n",
      "Speed: 1.1ms preprocess, 110.1ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 119.0ms\n",
      "Speed: 2.0ms preprocess, 119.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 105.4ms\n",
      "Speed: 1.0ms preprocess, 105.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 101.6ms\n",
      "Speed: 6.0ms preprocess, 101.6ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Gender, 117.8ms\n",
      "Speed: 0.0ms preprocess, 117.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 117.4ms\n",
      "Speed: 0.0ms preprocess, 117.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 107.1ms\n",
      "Speed: 0.0ms preprocess, 107.1ms inference, 6.1ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 123.8ms\n",
      "Speed: 0.0ms preprocess, 123.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 105.3ms\n",
      "Speed: 0.0ms preprocess, 105.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 114.4ms\n",
      "Speed: 2.0ms preprocess, 114.4ms inference, 4.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 112.0ms\n",
      "Speed: 0.0ms preprocess, 112.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 111.0ms\n",
      "Speed: 2.8ms preprocess, 111.0ms inference, 3.1ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 Gender, 113.1ms\n",
      "Speed: 0.0ms preprocess, 113.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 128.1ms\n",
      "Speed: 2.7ms preprocess, 128.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 116.4ms\n",
      "Speed: 0.0ms preprocess, 116.4ms inference, 3.2ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 104.1ms\n",
      "Speed: 4.3ms preprocess, 104.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 109.8ms\n",
      "Speed: 3.4ms preprocess, 109.8ms inference, 2.6ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 117.7ms\n",
      "Speed: 0.0ms preprocess, 117.7ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 109.4ms\n",
      "Speed: 0.0ms preprocess, 109.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 111.4ms\n",
      "Speed: 3.5ms preprocess, 111.4ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 120.3ms\n",
      "Speed: 0.0ms preprocess, 120.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 127.2ms\n",
      "Speed: 3.8ms preprocess, 127.2ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 115.6ms\n",
      "Speed: 4.3ms preprocess, 115.6ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 99.7ms\n",
      "Speed: 4.5ms preprocess, 99.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 116.5ms\n",
      "Speed: 0.0ms preprocess, 116.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 118.9ms\n",
      "Speed: 0.0ms preprocess, 118.9ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 107.7ms\n",
      "Speed: 3.8ms preprocess, 107.7ms inference, 4.6ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 118.7ms\n",
      "Speed: 0.0ms preprocess, 118.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 113.4ms\n",
      "Speed: 0.0ms preprocess, 113.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 1 Satyamav Jayate, 117.7ms\n",
      "Speed: 3.7ms preprocess, 117.7ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 107.5ms\n",
      "Speed: 2.1ms preprocess, 107.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 122.0ms\n",
      "Speed: 0.0ms preprocess, 122.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 96.6ms\n",
      "Speed: 0.0ms preprocess, 96.6ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 111.6ms\n",
      "Speed: 3.4ms preprocess, 111.6ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 111.0ms\n",
      "Speed: 0.0ms preprocess, 111.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 117.4ms\n",
      "Speed: 0.0ms preprocess, 117.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 111.8ms\n",
      "Speed: 0.0ms preprocess, 111.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 116.9ms\n",
      "Speed: 0.0ms preprocess, 116.9ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 104.7ms\n",
      "Speed: 7.7ms preprocess, 104.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 115.9ms\n",
      "Speed: 0.0ms preprocess, 115.9ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 106.7ms\n",
      "Speed: 0.0ms preprocess, 106.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 107.0ms\n",
      "Speed: 3.5ms preprocess, 107.0ms inference, 9.7ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 109.4ms\n",
      "Speed: 0.0ms preprocess, 109.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 123.9ms\n",
      "Speed: 0.0ms preprocess, 123.9ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 DOB, 1 Gender, 104.1ms\n",
      "Speed: 0.0ms preprocess, 104.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 112.1ms\n",
      "Speed: 1.0ms preprocess, 112.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Gender, 115.0ms\n",
      "Speed: 0.0ms preprocess, 115.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 112.5ms\n",
      "Speed: 0.0ms preprocess, 112.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 Name, 1 DOB, 1 Gender, 129.5ms\n",
      "Speed: 0.0ms preprocess, 129.5ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 127.9ms\n",
      "Speed: 4.3ms preprocess, 127.9ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 Name, 1 Gender, 116.7ms\n",
      "Speed: 0.0ms preprocess, 116.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 117.5ms\n",
      "Speed: 4.7ms preprocess, 117.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 DOB, 1 Gender, 116.6ms\n",
      "Speed: 10.8ms preprocess, 116.6ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 119.4ms\n",
      "Speed: 3.8ms preprocess, 119.4ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 Name, 1 DOB, 1 Gender, 100.5ms\n",
      "Speed: 3.1ms preprocess, 100.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 119.3ms\n",
      "Speed: 0.0ms preprocess, 119.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 DOB, 1 Gender, 115.7ms\n",
      "Speed: 1.1ms preprocess, 115.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 119.2ms\n",
      "Speed: 0.0ms preprocess, 119.2ms inference, 5.5ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 DOB, 1 Gender, 128.5ms\n",
      "Speed: 0.0ms preprocess, 128.5ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 111.5ms\n",
      "Speed: 4.9ms preprocess, 111.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 Name, 1 Gender, 108.4ms\n",
      "Speed: 7.4ms preprocess, 108.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 115.4ms\n",
      "Speed: 0.0ms preprocess, 115.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 Name, 1 DOB, 2 Genders, 125.1ms\n",
      "Speed: 2.3ms preprocess, 125.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 111.2ms\n",
      "Speed: 0.0ms preprocess, 111.2ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 Name, 1 DOB, 2 Genders, 108.2ms\n",
      "Speed: 7.9ms preprocess, 108.2ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 112.3ms\n",
      "Speed: 4.2ms preprocess, 112.3ms inference, 4.3ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 Name, 1 DOB, 2 Genders, 115.7ms\n",
      "Speed: 0.0ms preprocess, 115.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 120.8ms\n",
      "Speed: 4.4ms preprocess, 120.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 DOB, 1 Gender, 104.0ms\n",
      "Speed: 0.0ms preprocess, 104.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 109.3ms\n",
      "Speed: 3.0ms preprocess, 109.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 DOB, 2 Genders, 116.6ms\n",
      "Speed: 9.8ms preprocess, 116.6ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 121.1ms\n",
      "Speed: 6.2ms preprocess, 121.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 DOB, 2 Genders, 96.3ms\n",
      "Speed: 0.0ms preprocess, 96.3ms inference, 8.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 111.2ms\n",
      "Speed: 0.0ms preprocess, 111.2ms inference, 2.7ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 2 Genders, 105.7ms\n",
      "Speed: 6.3ms preprocess, 105.7ms inference, 7.4ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 107.9ms\n",
      "Speed: 0.0ms preprocess, 107.9ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 DOB, 116.8ms\n",
      "Speed: 0.0ms preprocess, 116.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 116.9ms\n",
      "Speed: 3.7ms preprocess, 116.9ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 Gender, 110.9ms\n",
      "Speed: 0.0ms preprocess, 110.9ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 2 PICs, 107.7ms\n",
      "Speed: 0.0ms preprocess, 107.7ms inference, 5.6ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 Name, 1 DOB, 1 Gender, 110.3ms\n",
      "Speed: 0.0ms preprocess, 110.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 123.0ms\n",
      "Speed: 3.2ms preprocess, 123.0ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 2 Genders, 117.0ms\n",
      "Speed: 0.0ms preprocess, 117.0ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 2 PICs, 114.9ms\n",
      "Speed: 0.0ms preprocess, 114.9ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 2 Genders, 120.8ms\n",
      "Speed: 0.0ms preprocess, 120.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 PIC, 120.4ms\n",
      "Speed: 0.0ms preprocess, 120.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 111.3ms\n",
      "Speed: 0.0ms preprocess, 111.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 111.9ms\n",
      "Speed: 2.8ms preprocess, 111.9ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 2 Genders, 126.7ms\n",
      "Speed: 0.0ms preprocess, 126.7ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 116.0ms\n",
      "Speed: 1.1ms preprocess, 116.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 125.0ms\n",
      "Speed: 8.7ms preprocess, 125.0ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 110.4ms\n",
      "Speed: 1.5ms preprocess, 110.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 2 Genders, 106.5ms\n",
      "Speed: 8.3ms preprocess, 106.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 116.3ms\n",
      "Speed: 0.0ms preprocess, 116.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 126.7ms\n",
      "Speed: 0.0ms preprocess, 126.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 117.5ms\n",
      "Speed: 1.7ms preprocess, 117.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 103.8ms\n",
      "Speed: 2.1ms preprocess, 103.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 122.1ms\n",
      "Speed: 0.0ms preprocess, 122.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 DOB, 2 Genders, 112.4ms\n",
      "Speed: 0.5ms preprocess, 112.4ms inference, 1.2ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 128.5ms\n",
      "Speed: 3.4ms preprocess, 128.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 GOV, 1 DOB, 1 Gender, 110.7ms\n",
      "Speed: 0.0ms preprocess, 110.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 120.3ms\n",
      "Speed: 0.0ms preprocess, 120.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Gender, 112.9ms\n",
      "Speed: 3.2ms preprocess, 112.9ms inference, 5.6ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 115.1ms\n",
      "Speed: 0.0ms preprocess, 115.1ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 109.9ms\n",
      "Speed: 0.0ms preprocess, 109.9ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 113.6ms\n",
      "Speed: 0.0ms preprocess, 113.6ms inference, 2.8ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 113.7ms\n",
      "Speed: 0.0ms preprocess, 113.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 117.7ms\n",
      "Speed: 0.0ms preprocess, 117.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 108.8ms\n",
      "Speed: 0.0ms preprocess, 108.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 117.4ms\n",
      "Speed: 0.0ms preprocess, 117.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 109.4ms\n",
      "Speed: 3.3ms preprocess, 109.4ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 118.3ms\n",
      "Speed: 0.9ms preprocess, 118.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 115.7ms\n",
      "Speed: 0.0ms preprocess, 115.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 106.6ms\n",
      "Speed: 5.8ms preprocess, 106.6ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 2 Genders, 108.5ms\n",
      "Speed: 2.4ms preprocess, 108.5ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 113.4ms\n",
      "Speed: 2.1ms preprocess, 113.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 104.3ms\n",
      "Speed: 0.0ms preprocess, 104.3ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 115.4ms\n",
      "Speed: 3.8ms preprocess, 115.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 1 Gender, 110.5ms\n",
      "Speed: 4.3ms preprocess, 110.5ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 122.4ms\n",
      "Speed: 0.0ms preprocess, 122.4ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 Name, 1 DOB, 1 Gender, 132.3ms\n",
      "Speed: 2.0ms preprocess, 132.3ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 117.7ms\n",
      "Speed: 0.0ms preprocess, 117.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 2 Genders, 113.8ms\n",
      "Speed: 10.7ms preprocess, 113.8ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 (no detections), 120.7ms\n",
      "Speed: 0.0ms preprocess, 120.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n",
      "\n",
      "0: 288x352 1 DOB, 2 Genders, 118.4ms\n",
      "Speed: 0.0ms preprocess, 118.4ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 352)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# model1 = YOLO('runs\\detect\\train5') \n",
    "# model2 = YOLO('runs\\detect\\train7')\n",
    "\n",
    "# Open video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run inference with both models\n",
    "    results1 = model2.predict(source=frame, conf=0.25)\n",
    "    results2 = model3.predict(source=frame, conf=0.25)\n",
    "\n",
    "    # Annotate frame with results from both models\n",
    "    annotated_frame1 = results1[0].plot()\n",
    "    annotated_frame2 = results2[0].plot()\n",
    "\n",
    "    # Combine the annotations (if needed)\n",
    "    combined_frame = cv2.addWeighted(annotated_frame1, 0.5, annotated_frame2, 0.5, 0)\n",
    "\n",
    "    # Display combined results\n",
    "    cv2.imshow('YOLO Combined Detection', combined_frame)\n",
    "\n",
    "    # Break the loop on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralyticsplus\n",
      "  Downloading ultralyticsplus-0.1.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub>=0.12.0 (from ultralyticsplus)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fire (from ultralyticsplus)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "     -------------------------------------- 87.2/87.2 kB 986.5 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of ultralyticsplus to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting ultralyticsplus\n",
      "  Downloading ultralyticsplus-0.0.29-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading ultralyticsplus-0.0.28-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting ultralytics<8.0.44,>=8.0.43 (from ultralyticsplus)\n",
      "  Downloading ultralytics-8.0.43-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting sahi<0.12.0,>=0.11.11 (from ultralyticsplus)\n",
      "  Downloading sahi-0.11.20-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from ultralyticsplus) (1.3.5)\n",
      "Collecting filelock (from huggingface-hub>=0.12.0->ultralyticsplus)\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting fsspec (from huggingface-hub>=0.12.0->ultralyticsplus)\n",
      "  Downloading fsspec-2023.1.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from huggingface-hub>=0.12.0->ultralyticsplus) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from huggingface-hub>=0.12.0->ultralyticsplus) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from huggingface-hub>=0.12.0->ultralyticsplus) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from huggingface-hub>=0.12.0->ultralyticsplus) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from huggingface-hub>=0.12.0->ultralyticsplus) (24.0)\n",
      "Collecting importlib-metadata (from huggingface-hub>=0.12.0->ultralyticsplus)\n",
      "  Downloading importlib_metadata-6.7.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: opencv-python<=4.10.0.84 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from sahi<0.12.0,>=0.11.11->ultralyticsplus) (4.10.0.84)\n",
      "Requirement already satisfied: shapely>=2.0.0 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from sahi<0.12.0,>=0.11.11->ultralyticsplus) (2.0.6)\n",
      "Requirement already satisfied: pillow>=8.2.0 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from sahi<0.12.0,>=0.11.11->ultralyticsplus) (9.5.0)\n",
      "Collecting pybboxes==0.1.6 (from sahi<0.12.0,>=0.11.11->ultralyticsplus)\n",
      "  Downloading pybboxes-0.1.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting terminaltables (from sahi<0.12.0,>=0.11.11->ultralyticsplus)\n",
      "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting click (from sahi<0.12.0,>=0.11.11->ultralyticsplus)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from sahi<0.12.0,>=0.11.11->ultralyticsplus) (1.21.6)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from ultralytics<8.0.44,>=8.0.43->ultralyticsplus) (3.5.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from ultralytics<8.0.44,>=8.0.43->ultralyticsplus) (1.7.3)\n",
      "Requirement already satisfied: torch>=1.7.0 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from ultralytics<8.0.44,>=8.0.43->ultralyticsplus) (1.13.1)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from ultralytics<8.0.44,>=8.0.43->ultralyticsplus) (0.14.1)\n",
      "Collecting tensorboard>=2.4.1 (from ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from ultralytics<8.0.44,>=8.0.43->ultralyticsplus) (0.12.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from ultralytics<8.0.44,>=8.0.43->ultralyticsplus) (6.1.0)\n",
      "Collecting thop>=0.1.1 (from ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: wheel>=0.38.0 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from ultralytics<8.0.44,>=8.0.43->ultralyticsplus) (0.42.0)\n",
      "Collecting sentry-sdk (from ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Downloading sentry_sdk-2.19.2-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from pandas->ultralyticsplus) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from pandas->ultralyticsplus) (2024.2)\n",
      "Collecting termcolor (from fire->ultralyticsplus)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics<8.0.44,>=8.0.43->ultralyticsplus) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics<8.0.44,>=8.0.43->ultralyticsplus) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics<8.0.44,>=8.0.43->ultralyticsplus) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics<8.0.44,>=8.0.43->ultralyticsplus) (3.1.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->ultralyticsplus) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from requests->huggingface-hub>=0.12.0->ultralyticsplus) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from requests->huggingface-hub>=0.12.0->ultralyticsplus) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from requests->huggingface-hub>=0.12.0->ultralyticsplus) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from requests->huggingface-hub>=0.12.0->ultralyticsplus) (2024.8.30)\n",
      "Collecting absl-py>=0.4 (from tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.24.3 (from tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Downloading grpcio-1.62.3-cp37-cp37m-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Downloading google_auth-2.37.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting protobuf<4,>=3.9.2 (from tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Downloading protobuf-3.20.3-cp37-cp37m-win_amd64.whl.metadata (699 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus) (68.0.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.12.0->ultralyticsplus) (0.4.6)\n",
      "Collecting zipp>=0.5 (from importlib-metadata->huggingface-hub>=0.12.0->ultralyticsplus)\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Downloading MarkupSafe-2.1.5-cp37-cp37m-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->ultralytics<8.0.44,>=8.0.43->ultralyticsplus)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading ultralyticsplus-0.0.28-py3-none-any.whl (11 kB)\n",
      "Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "   ---------------------------------------- 268.8/268.8 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading sahi-0.11.20-py3-none-any.whl (112 kB)\n",
      "   ---------------------------------------- 112.4/112.4 kB 6.4 MB/s eta 0:00:00\n",
      "Downloading pybboxes-0.1.6-py3-none-any.whl (24 kB)\n",
      "Downloading ultralytics-8.0.43-py3-none-any.whl (299 kB)\n",
      "   ---------------------------------------- 299.6/299.6 kB 4.7 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "   ---------------------------------------- 6.0/6.0 MB 19.1 MB/s eta 0:00:00\n",
      "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "   ---------------------------------------- 98.2/98.2 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "   ---------------------------------------- 143.0/143.0 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading sentry_sdk-2.19.2-py2.py3-none-any.whl (322 kB)\n",
      "   ---------------------------------------- 322.9/322.9 kB 9.8 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\n",
      "   ---------------------------------------- 209.8/209.8 kB 6.4 MB/s eta 0:00:00\n",
      "Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Downloading grpcio-1.62.3-cp37-cp37m-win_amd64.whl (4.5 MB)\n",
      "   ---------------------------------------- 4.5/4.5 MB 28.6 MB/s eta 0:00:00\n",
      "Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "   ---------------------------------------- 94.2/94.2 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading protobuf-3.20.3-cp37-cp37m-win_amd64.whl (905 kB)\n",
      "   --------------------------------------- 905.1/905.1 kB 28.0 MB/s eta 0:00:00\n",
      "Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "   --------------------------------------- 233.6/233.6 kB 14.0 MB/s eta 0:00:00\n",
      "Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp37-cp37m-win_amd64.whl (17 kB)\n",
      "Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 181.3/181.3 kB 5.5 MB/s eta 0:00:00\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 84.9/84.9 kB 4.7 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114262 sha256=99da5e9ea71023bdf8543b756641d382eae0136fa92975b1ea02476e4bc198bc\n",
      "  Stored in directory: c:\\users\\kiit0001\\appdata\\local\\pip\\cache\\wheels\\e2\\5a\\25\\38c86410f5a3b45d74468fd36ff0e1eb9a0c78edbb00169580\n",
      "Successfully built fire\n",
      "Installing collected packages: tensorboard-plugin-wit, zipp, terminaltables, termcolor, tensorboard-data-server, sentry-sdk, pybboxes, pyasn1, protobuf, oauthlib, MarkupSafe, grpcio, fsspec, filelock, cachetools, absl-py, werkzeug, thop, rsa, requests-oauthlib, pyasn1-modules, importlib-metadata, fire, markdown, huggingface-hub, google-auth, click, sahi, google-auth-oauthlib, tensorboard, ultralytics, ultralyticsplus\n",
      "  Attempting uninstall: ultralytics\n",
      "    Found existing installation: ultralytics 8.0.145\n",
      "    Uninstalling ultralytics-8.0.145:\n",
      "      Successfully uninstalled ultralytics-8.0.145\n",
      "Successfully installed MarkupSafe-2.1.5 absl-py-2.1.0 cachetools-5.5.0 click-8.1.8 filelock-3.12.2 fire-0.7.0 fsspec-2023.1.0 google-auth-2.37.0 google-auth-oauthlib-0.4.6 grpcio-1.62.3 huggingface-hub-0.16.4 importlib-metadata-6.7.0 markdown-3.4.4 oauthlib-3.2.2 protobuf-3.20.3 pyasn1-0.5.1 pyasn1-modules-0.3.0 pybboxes-0.1.6 requests-oauthlib-2.0.0 rsa-4.9 sahi-0.11.20 sentry-sdk-2.19.2 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 termcolor-2.3.0 terminaltables-3.1.10 thop-0.1.1.post2209072238 ultralytics-8.0.43 ultralyticsplus-0.0.28 werkzeug-2.2.3 zipp-3.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralyticsplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.43  Python-3.7.3 torch-1.13.1+cpu CPU\n",
      "Model summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "image 1/1 C:\\Users\\KIIT0001\\ocr\\ocr_id\\samples\\new_preprocessed\\adhar.jpg: 416x640 (no detections), 191.0ms\n",
      "Speed: 2.0ms preprocess, 191.0ms inference, 8.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], size=(0, 6))\n"
     ]
    }
   ],
   "source": [
    "from ultralyticsplus import render_result\n",
    "\n",
    "model4.overrides['conf'] = 0 # NMS confidence threshold\n",
    "model4.overrides['iou'] = 0.45  # NMS IoU threshold\n",
    "model4.overrides['agnostic_nms'] = False  # NMS class-agnostic\n",
    "model4.overrides['max_det'] = 1000  # maximum number of detections per image\n",
    "\n",
    "# set image\n",
    "image = 'adhar.jpg'\n",
    "\n",
    "# perform inference\n",
    "results = model4.predict(image)\n",
    "\n",
    "# observe results\n",
    "print(results[0].boxes)\n",
    "render = render_result(model=model4, image=image, result=results[0])\n",
    "render.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_labels_from_txt(directory, label_no):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\"txt\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            # Remove lines with the specified label\n",
    "            updated_lines = [line for line in lines if not line.startswith(f\"{label_no} \")]\n",
    "\n",
    "            # Write back the filtered lines\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.writelines(updated_lines)\n",
    "\n",
    "# Directory containing the YOLO txt files\n",
    "txt_files_directory = \"train4/labels\"\n",
    "\n",
    "# Label to be removed\n",
    "label_to_delete = \"1\"\n",
    "\n",
    "# Execute the function\n",
    "delete_labels_from_txt(txt_files_directory, label_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def swap_labels_yolo(txt_file_path, label_map):\n",
    "    \"\"\"\n",
    "    Swaps labels in a YOLO format .txt file based on a label mapping.\n",
    "\n",
    "    :param txt_file_path: Path to the YOLO .txt file.\n",
    "    :param label_map: Dictionary mapping old labels to new labels.\n",
    "    \"\"\"\n",
    "    with open(txt_file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 5:  # Ensure valid YOLO format\n",
    "            class_id = parts[0]\n",
    "            if class_id in label_map:\n",
    "                parts[0] = str(label_map[class_id])  # Swap label\n",
    "            new_lines.append(\" \".join(parts))\n",
    "\n",
    "    # Write updated lines back to the file\n",
    "    with open(txt_file_path, \"w\") as file:\n",
    "        file.write(\"\\n\".join(new_lines) + \"\\n\")\n",
    "\n",
    "def process_folder(folder_path, label_map):\n",
    "    \"\"\"\n",
    "    Processes all .txt files in a folder to swap labels.\n",
    "\n",
    "    :param folder_path: Path to the folder containing YOLO .txt files.\n",
    "    :param label_map: Dictionary mapping old labels to new labels.\n",
    "    \"\"\"\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            txt_file_path = os.path.join(folder_path, file_name)\n",
    "            swap_labels_yolo(txt_file_path, label_map)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"train4/labels\"  # Replace with your folder path\n",
    "    label_map = {\n",
    "        \"2\": \"1\",  # Swap label 1 with label 2\n",
    "        \"3\": \"2\",\n",
    "        \"4\": \"3\" # Swap label 2 with label 1\n",
    "    }\n",
    "    process_folder(folder_path, label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KIIT0001\\ocr\\ocrd\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model5 = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.43  Python-3.7.3 torch-1.13.1+cpu CPU\n",
      "\u001b[34m\u001b[1myolo\\engine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data5.yaml, epochs=50, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, min_memory=False, overlap_mask=True, mask_ratio=4, dropout=False, val=True, split=val, save_json=False, save_hybrid=False, conf=0.001, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=ultralytics/assets/, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs\\detect\\train15\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.Detect                [4, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011628 parameters, 3011612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.001), 63 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\KIIT0001\\ocr\\ocr_id\\samples\\new_preprocessed\\train4\\labels... 195 images, 2 backgrounds, 0 corrupt: 100%|██████████| 197/197 [00:00<00:00, 658.69it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\KIIT0001\\ocr\\ocr_id\\samples\\new_preprocessed\\train4\\labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\KIIT0001\\ocr\\ocr_id\\samples\\new_preprocessed\\valid4\\labels... 18 images, 0 backgrounds, 0 corrupt: 100%|██████████| 18/18 [00:00<00:00, 1110.37it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\KIIT0001\\ocr\\ocr_id\\samples\\new_preprocessed\\valid4\\labels.cache\n",
      "Plotting labels to runs\\detect\\train15\\labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train15\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/50         0G      2.039      4.161      1.597         18        640: 100%|██████████| 13/13 [04:00<00:00, 18.48s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.62s/it]\n",
      "                   all         18         72    0.00267      0.222    0.00492    0.00195\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/50         0G      1.411      3.753      1.204         30        640: 100%|██████████| 13/13 [02:53<00:00, 13.38s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.49s/it]\n",
      "                   all         18         72    0.00934      0.708      0.143     0.0751\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/50         0G      1.397      3.293      1.168         25        640: 100%|██████████| 13/13 [01:34<00:00,  7.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.73s/it]\n",
      "                   all         18         72     0.0151          1      0.301      0.154\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/50         0G      1.503      2.704      1.204         25        640: 100%|██████████| 13/13 [03:22<00:00, 15.55s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:11<00:00, 11.11s/it]\n",
      "                   all         18         72      0.015      0.972      0.438      0.182\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/50         0G      1.469      2.383      1.207         19        640: 100%|██████████| 13/13 [04:10<00:00, 19.26s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.76s/it]\n",
      "                   all         18         72      0.422      0.403      0.479      0.303\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/50         0G      1.463      2.234      1.196         22        640: 100%|██████████| 13/13 [02:02<00:00,  9.46s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.88s/it]\n",
      "                   all         18         72      0.972      0.175      0.549      0.329\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/50         0G      1.456      2.139      1.183         30        640: 100%|██████████| 13/13 [01:43<00:00,  7.98s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.31s/it]\n",
      "                   all         18         72      0.378      0.791      0.501       0.31\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/50         0G       1.53      2.023      1.201         21        640: 100%|██████████| 13/13 [01:35<00:00,  7.34s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.13s/it]\n",
      "                   all         18         72      0.715      0.455      0.638       0.33\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/50         0G      1.504      1.998      1.219         17        640: 100%|██████████| 13/13 [01:50<00:00,  8.51s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.49s/it]\n",
      "                   all         18         72      0.822      0.644      0.889      0.515\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/50         0G      1.471      1.861      1.204         23        640: 100%|██████████| 13/13 [02:02<00:00,  9.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.55s/it]\n",
      "                   all         18         72      0.899      0.497      0.834      0.471\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/50         0G      1.417      1.795      1.213         21        640: 100%|██████████| 13/13 [02:04<00:00,  9.59s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.62s/it]\n",
      "                   all         18         72      0.798      0.861      0.932      0.544\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/50         0G      1.414      1.689      1.178         28        640: 100%|██████████| 13/13 [02:03<00:00,  9.49s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.42s/it]\n",
      "                   all         18         72      0.811      0.795      0.895      0.506\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/50         0G      1.413      1.615      1.185         31        640: 100%|██████████| 13/13 [02:01<00:00,  9.34s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.50s/it]\n",
      "                   all         18         72       0.94      0.914      0.953      0.527\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/50         0G      1.427       1.59      1.188         19        640: 100%|██████████| 13/13 [02:06<00:00,  9.75s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.72s/it]\n",
      "                   all         18         72      0.916      0.903      0.948       0.58\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/50         0G      1.332      1.489      1.169         33        640: 100%|██████████| 13/13 [02:02<00:00,  9.40s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.48s/it]\n",
      "                   all         18         72      0.924      0.929      0.975      0.553\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/50         0G      1.408       1.54       1.21         35        640: 100%|██████████| 13/13 [01:48<00:00,  8.31s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.08s/it]\n",
      "                   all         18         72      0.939      0.917      0.954      0.541\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/50         0G      1.405      1.436      1.175         27        640: 100%|██████████| 13/13 [01:24<00:00,  6.53s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.73s/it]\n",
      "                   all         18         72      0.916      0.986      0.967      0.572\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/50         0G      1.345       1.38      1.158         25        640: 100%|██████████| 13/13 [01:28<00:00,  6.78s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.07s/it]\n",
      "                   all         18         72      0.971      0.963      0.971      0.612\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/50         0G       1.36      1.403      1.171         26        640: 100%|██████████| 13/13 [01:25<00:00,  6.60s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.43s/it]\n",
      "                   all         18         72      0.925      0.931      0.959      0.545\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/50         0G       1.38      1.412      1.179         44        640: 100%|██████████| 13/13 [01:19<00:00,  6.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.81s/it]\n",
      "                   all         18         72       0.94      0.986      0.975      0.607\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/50         0G       1.33      1.373      1.161         35        640: 100%|██████████| 13/13 [01:18<00:00,  6.03s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.92s/it]\n",
      "                   all         18         72      0.976      0.953      0.974      0.584\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/50         0G      1.257      1.257      1.126         39        640: 100%|██████████| 13/13 [01:59<00:00,  9.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.50s/it]\n",
      "                   all         18         72      0.962      0.986      0.975      0.592\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/50         0G      1.321      1.251      1.145         25        640: 100%|██████████| 13/13 [02:02<00:00,  9.41s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.48s/it]\n",
      "                   all         18         72      0.962       0.95       0.97      0.553\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/50         0G      1.276      1.184      1.155         30        640: 100%|██████████| 13/13 [02:00<00:00,  9.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.51s/it]\n",
      "                   all         18         72      0.944      0.971      0.978      0.566\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/50         0G      1.285      1.224       1.15         24        640: 100%|██████████| 13/13 [02:02<00:00,  9.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.32s/it]\n",
      "                   all         18         72      0.944      0.964      0.974      0.478\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      26/50         0G      1.307       1.18      1.142         15        640: 100%|██████████| 13/13 [01:59<00:00,  9.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.45s/it]\n",
      "                   all         18         72      0.939      0.986      0.977      0.533\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      27/50         0G      1.266      1.183      1.128         28        640: 100%|██████████| 13/13 [02:01<00:00,  9.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.86s/it]\n",
      "                   all         18         72      0.958      0.986      0.977      0.598\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      28/50         0G      1.243      1.144      1.115         26        640: 100%|██████████| 13/13 [01:18<00:00,  6.04s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.87s/it]\n",
      "                   all         18         72      0.954      0.934      0.973      0.524\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      29/50         0G      1.234      1.092      1.106         30        640: 100%|██████████| 13/13 [01:28<00:00,  6.80s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.18s/it]\n",
      "                   all         18         72      0.966      0.986      0.976       0.58\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      30/50         0G      1.241      1.132      1.124         36        640: 100%|██████████| 13/13 [01:54<00:00,  8.82s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.13s/it]\n",
      "                   all         18         72       0.96      0.986      0.976      0.609\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      31/50         0G      1.214      1.087      1.129         24        640: 100%|██████████| 13/13 [01:55<00:00,  8.89s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.30s/it]\n",
      "                   all         18         72      0.955      0.962      0.977      0.624\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      32/50         0G      1.223      1.117      1.158         16        640: 100%|██████████| 13/13 [01:40<00:00,  7.76s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.90s/it]\n",
      "                   all         18         72       0.96      0.986      0.981      0.593\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      33/50         0G      1.211      1.071      1.106         37        640: 100%|██████████| 13/13 [01:20<00:00,  6.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.85s/it]\n",
      "                   all         18         72      0.955      0.986      0.981      0.618\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      34/50         0G      1.209      1.036      1.111         38        640: 100%|██████████| 13/13 [01:50<00:00,  8.52s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.31s/it]\n",
      "                   all         18         72      0.964      0.986      0.977      0.616\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      35/50         0G      1.198       1.05      1.097         29        640: 100%|██████████| 13/13 [02:00<00:00,  9.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.28s/it]\n",
      "                   all         18         72       0.96      0.986      0.975      0.603\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      36/50         0G      1.206      1.049      1.109         41        640: 100%|██████████| 13/13 [02:00<00:00,  9.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.28s/it]\n",
      "                   all         18         72      0.952      0.986      0.976      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      37/50         0G      1.137     0.9534      1.103         24        640: 100%|██████████| 13/13 [02:00<00:00,  9.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.40s/it]\n",
      "                   all         18         72      0.977       0.96      0.975      0.587\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      38/50         0G      1.164     0.9902      1.098         20        640: 100%|██████████| 13/13 [02:01<00:00,  9.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.43s/it]\n",
      "                   all         18         72      0.951      0.986      0.975      0.572\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      39/50         0G      1.104     0.9576      1.071         29        640: 100%|██████████| 13/13 [02:02<00:00,  9.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.34s/it]\n",
      "                   all         18         72      0.965      0.986      0.975      0.599\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      40/50         0G      1.175      1.005      1.116         30        640: 100%|██████████| 13/13 [02:00<00:00,  9.26s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.40s/it]\n",
      "                   all         18         72      0.961      0.986      0.976      0.631\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      41/50         0G      1.086     0.9555      1.098         18        640: 100%|██████████| 13/13 [01:55<00:00,  8.91s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.25s/it]\n",
      "                   all         18         72      0.963      0.986      0.981      0.599\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      42/50         0G      1.113     0.9291      1.116         20        640: 100%|██████████| 13/13 [01:56<00:00,  8.93s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.43s/it]\n",
      "                   all         18         72      0.962      0.986      0.977       0.58\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      43/50         0G      1.079     0.9037      1.104         20        640: 100%|██████████| 13/13 [01:57<00:00,  9.02s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.24s/it]\n",
      "                   all         18         72       0.97      0.986      0.976      0.599\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      44/50         0G      1.082     0.9232      1.089         20        640: 100%|██████████| 13/13 [01:20<00:00,  6.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.85s/it]\n",
      "                   all         18         72       0.97      0.986      0.977      0.607\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      45/50         0G      1.049     0.8794       1.09         20        640: 100%|██████████| 13/13 [01:18<00:00,  6.01s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.27s/it]\n",
      "                   all         18         72      0.977      0.986      0.978      0.585\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      46/50         0G       1.08     0.8731      1.092         17        640: 100%|██████████| 13/13 [01:42<00:00,  7.91s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.86s/it]\n",
      "                   all         18         72      0.979      0.986      0.975      0.588\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      47/50         0G      1.033     0.8717      1.084         20        640: 100%|██████████| 13/13 [01:29<00:00,  6.86s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.84s/it]\n",
      "                   all         18         72       0.98      0.986      0.979      0.594\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      48/50         0G      1.053     0.8631      1.082         17        640: 100%|██████████| 13/13 [01:14<00:00,  5.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.81s/it]\n",
      "                   all         18         72      0.978      0.986       0.98      0.593\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      49/50         0G      1.046     0.8462      1.078         20        640: 100%|██████████| 13/13 [01:19<00:00,  6.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.83s/it]\n",
      "                   all         18         72      0.979      0.986       0.98      0.604\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      50/50         0G      1.029     0.8392      1.084         19        640: 100%|██████████| 13/13 [01:19<00:00,  6.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.88s/it]\n",
      "                   all         18         72      0.978      0.986       0.98      0.612\n",
      "\n",
      "50 epochs completed in 1.657 hours.\n",
      "Optimizer stripped from runs\\detect\\train15\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train15\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train15\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.43  Python-3.7.3 torch-1.13.1+cpu CPU\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.30s/it]\n",
      "                   all         18         72      0.961      0.986      0.976      0.631\n",
      "                   DOB         18         18       0.91      0.944      0.921      0.532\n",
      "           Income Logo         18         18      0.973          1      0.995      0.664\n",
      "                  Name         18         18      0.986          1      0.995      0.674\n",
      "            Pan Number         18         18      0.975          1      0.995      0.654\n",
      "Speed: 1.8ms preprocess, 112.4ms inference, 0.0ms loss, 4.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train15\u001b[0m\n",
      "Results saved to \u001b[1mruns\\detect\\train15\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model5.train(\n",
    "    data='data5.yaml',\n",
    "    epochs=50,\n",
    "    batch= 16,\n",
    "    imgsz= 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 219.5ms\n",
      "Speed: 2.0ms preprocess, 219.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 221.7ms\n",
      "Speed: 3.3ms preprocess, 221.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 185.5ms\n",
      "Speed: 1.3ms preprocess, 185.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 186.2ms\n",
      "Speed: 1.0ms preprocess, 186.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 184.9ms\n",
      "Speed: 1.0ms preprocess, 184.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 173.2ms\n",
      "Speed: 1.0ms preprocess, 173.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 169.8ms\n",
      "Speed: 1.0ms preprocess, 169.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 175.1ms\n",
      "Speed: 1.0ms preprocess, 175.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 169.6ms\n",
      "Speed: 1.0ms preprocess, 169.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 168.7ms\n",
      "Speed: 2.0ms preprocess, 168.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 176.7ms\n",
      "Speed: 1.0ms preprocess, 176.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 196.5ms\n",
      "Speed: 1.0ms preprocess, 196.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 175.9ms\n",
      "Speed: 2.0ms preprocess, 175.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 168.7ms\n",
      "Speed: 1.0ms preprocess, 168.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 DOBs, 175.2ms\n",
      "Speed: 1.0ms preprocess, 175.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 212.1ms\n",
      "Speed: 2.3ms preprocess, 212.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 DOBs, 201.7ms\n",
      "Speed: 2.0ms preprocess, 201.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 203.9ms\n",
      "Speed: 1.0ms preprocess, 203.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 198.2ms\n",
      "Speed: 1.0ms preprocess, 198.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 197.0ms\n",
      "Speed: 1.0ms preprocess, 197.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 202.2ms\n",
      "Speed: 2.0ms preprocess, 202.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 208.3ms\n",
      "Speed: 2.6ms preprocess, 208.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 175.8ms\n",
      "Speed: 2.0ms preprocess, 175.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 182.4ms\n",
      "Speed: 1.0ms preprocess, 182.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 185.0ms\n",
      "Speed: 1.0ms preprocess, 185.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 179.7ms\n",
      "Speed: 1.0ms preprocess, 179.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 DOBs, 187.2ms\n",
      "Speed: 1.0ms preprocess, 187.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 186.7ms\n",
      "Speed: 2.3ms preprocess, 186.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 189.5ms\n",
      "Speed: 2.0ms preprocess, 189.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 180.2ms\n",
      "Speed: 1.0ms preprocess, 180.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 217.9ms\n",
      "Speed: 2.0ms preprocess, 217.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 193.2ms\n",
      "Speed: 1.0ms preprocess, 193.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 204.3ms\n",
      "Speed: 2.0ms preprocess, 204.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 183.7ms\n",
      "Speed: 2.3ms preprocess, 183.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Name, 181.4ms\n",
      "Speed: 1.0ms preprocess, 181.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 198.1ms\n",
      "Speed: 3.0ms preprocess, 198.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 210.1ms\n",
      "Speed: 2.0ms preprocess, 210.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Name, 1 Pan Number, 195.2ms\n",
      "Speed: 2.0ms preprocess, 195.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 195.0ms\n",
      "Speed: 1.0ms preprocess, 195.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 211.7ms\n",
      "Speed: 2.0ms preprocess, 211.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 186.1ms\n",
      "Speed: 1.0ms preprocess, 186.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 189.7ms\n",
      "Speed: 2.0ms preprocess, 189.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Number, 199.1ms\n",
      "Speed: 1.0ms preprocess, 199.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 220.0ms\n",
      "Speed: 2.0ms preprocess, 220.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Number, 206.0ms\n",
      "Speed: 1.0ms preprocess, 206.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Number, 207.3ms\n",
      "Speed: 2.0ms preprocess, 207.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Pan Number, 185.1ms\n",
      "Speed: 1.0ms preprocess, 185.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 192.7ms\n",
      "Speed: 2.0ms preprocess, 192.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Name, 169.2ms\n",
      "Speed: 2.0ms preprocess, 169.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 178.8ms\n",
      "Speed: 1.0ms preprocess, 178.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 172.9ms\n",
      "Speed: 1.0ms preprocess, 172.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 176.4ms\n",
      "Speed: 1.0ms preprocess, 176.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "# Open a webcam feed or video file\n",
    "import cv2\n",
    "import os\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for webcam, or replace with video file path\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run the model on the frame\n",
    "    results = model5.predict(source=frame, conf=0.25)\n",
    "\n",
    "    # Render the results on the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow('YOLO Detection', annotated_frame)\n",
    "\n",
    "    # Break on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "for box in results[0].boxes:\n",
    "    print(f\"Class: {box.cls}, Confidence: {box.conf}, Coordinates: {box.xyxy}\")\n",
    "    # print(box.data)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 225.0ms\n",
      "Speed: 7.8ms preprocess, 225.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 238.0ms\n",
      "Speed: 1.0ms preprocess, 238.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 174.2ms\n",
      "Speed: 1.0ms preprocess, 174.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 174.5ms\n",
      "Speed: 1.0ms preprocess, 174.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 168.0ms\n",
      "Speed: 1.0ms preprocess, 168.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 170.2ms\n",
      "Speed: 1.0ms preprocess, 170.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 165.9ms\n",
      "Speed: 1.0ms preprocess, 165.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 170.4ms\n",
      "Speed: 2.0ms preprocess, 170.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 176.6ms\n",
      "Speed: 1.0ms preprocess, 176.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 165.2ms\n",
      "Speed: 1.0ms preprocess, 165.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 190.0ms\n",
      "Speed: 1.0ms preprocess, 190.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 172.5ms\n",
      "Speed: 2.1ms preprocess, 172.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 162.8ms\n",
      "Speed: 3.0ms preprocess, 162.8ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 DOBs, 173.1ms\n",
      "Speed: 1.0ms preprocess, 173.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.47856]), Coordinates: tensor([[245., 178., 258., 228.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.30810]), Coordinates: tensor([[292., 238., 307., 283.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.29683]), Coordinates: tensor([[260., 235., 273., 288.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.25835]), Coordinates: tensor([[244., 177., 257., 227.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 172.9ms\n",
      "Speed: 2.0ms preprocess, 172.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 172.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.31197]), Coordinates: tensor([[244., 178., 257., 227.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.0ms preprocess, 172.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 191.0ms\n",
      "Speed: 3.0ms preprocess, 191.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 182.4ms\n",
      "Speed: 2.0ms preprocess, 182.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 164.7ms\n",
      "Speed: 1.7ms preprocess, 164.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 165.4ms\n",
      "Speed: 2.0ms preprocess, 165.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 161.0ms\n",
      "Speed: 2.0ms preprocess, 161.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 DOBs, 169.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.40814]), Coordinates: tensor([[283., 235., 299., 288.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.0ms preprocess, 169.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 DOBs, 168.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.63854]), Coordinates: tensor([[244., 240., 261., 295.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.39112]), Coordinates: tensor([[228., 173., 241., 225.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.29541]), Coordinates: tensor([[283., 240., 298., 295.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.0ms preprocess, 168.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 183.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.33330]), Coordinates: tensor([[228., 175., 241., 236.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.28608]), Coordinates: tensor([[283., 243., 301., 308.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.27883]), Coordinates: tensor([[244., 242., 263., 301.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.4ms preprocess, 183.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 181.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.47154]), Coordinates: tensor([[243., 248., 261., 309.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.1ms preprocess, 181.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 185.3ms\n",
      "Speed: 2.0ms preprocess, 185.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 169.0ms\n",
      "Speed: 1.0ms preprocess, 169.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 182.3ms\n",
      "Speed: 1.0ms preprocess, 182.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 183.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.37289]), Coordinates: tensor([[479., 226., 501., 324.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.0ms preprocess, 183.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 186.2ms\n",
      "Speed: 3.3ms preprocess, 186.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 181.5ms\n",
      "Speed: 2.0ms preprocess, 181.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Income Logo, 190.4ms\n",
      "Speed: 1.0ms preprocess, 190.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Name, 172.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([1.]), Confidence: tensor([0.25100]), Coordinates: tensor([[442., 151., 462., 352.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.5ms preprocess, 172.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Name, 180.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.46758]), Coordinates: tensor([[500., 209., 518., 362.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.0ms preprocess, 180.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 167.6ms\n",
      "Speed: 2.2ms preprocess, 167.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.37483]), Coordinates: tensor([[494., 204., 513., 351.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 189.7ms\n",
      "Speed: 1.1ms preprocess, 189.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 178.5ms\n",
      "Speed: 2.0ms preprocess, 178.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 174.0ms\n",
      "Speed: 1.0ms preprocess, 174.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 182.1ms\n",
      "Speed: 1.0ms preprocess, 182.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 169.9ms\n",
      "Speed: 1.0ms preprocess, 169.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 165.4ms\n",
      "Speed: 2.0ms preprocess, 165.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 180.9ms\n",
      "Speed: 1.0ms preprocess, 180.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 183.1ms\n",
      "Speed: 2.5ms preprocess, 183.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 170.4ms\n",
      "Speed: 1.0ms preprocess, 170.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 195.7ms\n",
      "Speed: 2.0ms preprocess, 195.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 236.2ms\n",
      "Speed: 2.0ms preprocess, 236.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 213.6ms\n",
      "Speed: 1.9ms preprocess, 213.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 215.3ms\n",
      "Speed: 1.0ms preprocess, 215.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 210.8ms\n",
      "Speed: 1.5ms preprocess, 210.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 202.6ms\n",
      "Speed: 2.0ms preprocess, 202.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 186.9ms\n",
      "Speed: 2.0ms preprocess, 186.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Name, 174.4ms\n",
      "Speed: 1.0ms preprocess, 174.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Names, 166.4ms\n",
      "Speed: 2.0ms preprocess, 166.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.29700]), Coordinates: tensor([[348., 164., 370., 279.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 Names, 183.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.71223]), Coordinates: tensor([[333., 167., 352., 274.]])\n",
      "Class: tensor([2.]), Confidence: tensor([0.41847]), Coordinates: tensor([[376., 179., 394., 331.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.0ms preprocess, 183.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Name, 187.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.46792]), Coordinates: tensor([[333., 169., 350., 274.]])\n",
      "Class: tensor([2.]), Confidence: tensor([0.32822]), Coordinates: tensor([[376., 179., 395., 333.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.0ms preprocess, 187.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Names, 177.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.44337]), Coordinates: tensor([[376., 184., 394., 336.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.0ms preprocess, 177.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.47414]), Coordinates: tensor([[379., 184., 396., 334.]])\n",
      "Class: tensor([2.]), Confidence: tensor([0.29331]), Coordinates: tensor([[333., 168., 350., 298.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 Names, 183.3ms\n",
      "Speed: 2.0ms preprocess, 183.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 4 Names, 196.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.41667]), Coordinates: tensor([[335., 169., 352., 289.]])\n",
      "Class: tensor([2.]), Confidence: tensor([0.41563]), Coordinates: tensor([[377., 182., 395., 334.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.0ms preprocess, 196.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.41436]), Coordinates: tensor([[336., 171., 354., 278.]])\n",
      "Class: tensor([2.]), Confidence: tensor([0.33437]), Coordinates: tensor([[397., 183., 428., 340.]])\n",
      "Class: tensor([2.]), Confidence: tensor([0.33212]), Coordinates: tensor([[378., 184., 395., 335.]])\n",
      "Class: tensor([2.]), Confidence: tensor([0.25666]), Coordinates: tensor([[406., 184., 429., 339.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 Names, 209.0ms\n",
      "Speed: 1.0ms preprocess, 209.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.52732]), Coordinates: tensor([[379., 186., 396., 338.]])\n",
      "Class: tensor([2.]), Confidence: tensor([0.33591]), Coordinates: tensor([[336., 172., 356., 283.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Name, 203.8ms\n",
      "Speed: 1.0ms preprocess, 203.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.52138]), Coordinates: tensor([[379., 188., 396., 339.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 Names, 195.6ms\n",
      "Speed: 2.0ms preprocess, 195.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Name, 187.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.61557]), Coordinates: tensor([[334., 172., 352., 273.]])\n",
      "Class: tensor([2.]), Confidence: tensor([0.60104]), Coordinates: tensor([[404., 189., 429., 340.]])\n",
      "Class: tensor([2.]), Confidence: tensor([0.43532]), Coordinates: tensor([[377., 187., 395., 337.]])\n",
      "Class: tensor([2.]), Confidence: tensor([0.28596]), Coordinates: tensor([[395., 187., 427., 338.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.0ms preprocess, 187.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.29806]), Coordinates: tensor([[329., 171., 349., 273.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 193.0ms\n",
      "Speed: 13.0ms preprocess, 193.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 176.3ms\n",
      "Speed: 2.0ms preprocess, 176.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 180.9ms\n",
      "Speed: 1.0ms preprocess, 180.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 178.3ms\n",
      "Speed: 2.0ms preprocess, 178.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 1 Pan Number, 186.0ms\n",
      "Speed: 1.5ms preprocess, 186.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.28751]), Coordinates: tensor([[383., 268., 405., 337.]])\n",
      "Class: tensor([3.]), Confidence: tensor([0.25035]), Coordinates: tensor([[ 38., 285.,  61., 378.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 1 Pan Number, 187.2ms\n",
      "Speed: 2.0ms preprocess, 187.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([3.]), Confidence: tensor([0.43071]), Coordinates: tensor([[ 14., 285.,  36., 385.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.34295]), Coordinates: tensor([[370., 351., 392., 416.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Name, 1 Pan Number, 224.9ms\n",
      "Speed: 1.5ms preprocess, 224.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([3.]), Confidence: tensor([0.48412]), Coordinates: tensor([[  0., 287.,  24., 387.]])\n",
      "Class: tensor([2.]), Confidence: tensor([0.32116]), Coordinates: tensor([[ 47., 161.,  69., 237.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 1 Pan Number, 247.5ms\n",
      "Speed: 3.0ms preprocess, 247.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([3.]), Confidence: tensor([0.79040]), Coordinates: tensor([[  0., 281.,  22., 384.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.27145]), Coordinates: tensor([[358., 347., 381., 412.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Name, 1 Pan Number, 220.0ms\n",
      "Speed: 2.0ms preprocess, 220.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([3.]), Confidence: tensor([0.50563]), Coordinates: tensor([[  0., 271.,  12., 366.]])\n",
      "Class: tensor([2.]), Confidence: tensor([0.37035]), Coordinates: tensor([[409., 344., 432., 476.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Pan Number, 200.7ms\n",
      "Speed: 2.0ms preprocess, 200.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([3.]), Confidence: tensor([0.47084]), Coordinates: tensor([[  0., 265.,  13., 365.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 1 Pan Number, 255.7ms\n",
      "Speed: 4.1ms preprocess, 255.7ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([3.]), Confidence: tensor([0.71060]), Coordinates: tensor([[  0., 270.,  22., 367.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.31977]), Coordinates: tensor([[348., 331., 369., 387.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 1 Name, 216.3ms\n",
      "Speed: 2.0ms preprocess, 216.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.46102]), Coordinates: tensor([[424., 262., 447., 472.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.35349]), Coordinates: tensor([[355., 323., 377., 384.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 198.5ms\n",
      "Speed: 2.0ms preprocess, 198.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 196.6ms\n",
      "Speed: 1.0ms preprocess, 196.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 217.4ms\n",
      "Speed: 3.0ms preprocess, 217.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 203.0ms\n",
      "Speed: 2.0ms preprocess, 203.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 1 Name, 236.1ms\n",
      "Speed: 2.0ms preprocess, 236.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.58631]), Coordinates: tensor([[471., 260., 493., 402.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.52503]), Coordinates: tensor([[129., 275., 151., 367.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Name, 196.1ms\n",
      "Speed: 2.0ms preprocess, 196.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 198.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.37347]), Coordinates: tensor([[473., 243., 493., 386.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.0ms preprocess, 198.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 190.8ms\n",
      "Speed: 2.0ms preprocess, 190.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 220.5ms\n",
      "Speed: 2.0ms preprocess, 220.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 200.3ms\n",
      "Speed: 2.0ms preprocess, 200.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 223.4ms\n",
      "Speed: 2.0ms preprocess, 223.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 198.4ms\n",
      "Speed: 1.0ms preprocess, 198.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 201.6ms\n",
      "Speed: 1.0ms preprocess, 201.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 239.8ms\n",
      "Speed: 1.0ms preprocess, 239.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 201.1ms\n",
      "Speed: 2.0ms preprocess, 201.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 224.2ms\n",
      "Speed: 1.0ms preprocess, 224.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 234.1ms\n",
      "Speed: 2.0ms preprocess, 234.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 208.6ms\n",
      "Speed: 2.9ms preprocess, 208.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 217.0ms\n",
      "Speed: 1.0ms preprocess, 217.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Name, 206.3ms\n",
      "Speed: 1.0ms preprocess, 206.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.48621]), Coordinates: tensor([[471., 253., 497., 388.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Name, 243.6ms\n",
      "Speed: 2.0ms preprocess, 243.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.37643]), Coordinates: tensor([[ 56., 245.,  79., 350.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 227.1ms\n",
      "Speed: 3.5ms preprocess, 227.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 208.7ms\n",
      "Speed: 2.0ms preprocess, 208.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 249.8ms\n",
      "Speed: 2.0ms preprocess, 249.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 204.9ms\n",
      "Speed: 2.0ms preprocess, 204.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 240.7ms\n",
      "Speed: 2.0ms preprocess, 240.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 227.8ms\n",
      "Speed: 2.0ms preprocess, 227.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Name, 230.3ms\n",
      "Speed: 2.0ms preprocess, 230.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.53199]), Coordinates: tensor([[472., 200., 492., 327.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 256.5ms\n",
      "Speed: 3.0ms preprocess, 256.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 1 Name, 204.3ms\n",
      "Speed: 3.4ms preprocess, 204.3ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.55954]), Coordinates: tensor([[473., 192., 493., 312.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.31071]), Coordinates: tensor([[221., 113., 239., 177.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 234.9ms\n",
      "Speed: 2.0ms preprocess, 234.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 206.1ms\n",
      "Speed: 2.0ms preprocess, 206.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 208.5ms\n",
      "Speed: 2.0ms preprocess, 208.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 242.3ms\n",
      "Speed: 1.0ms preprocess, 242.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 213.2ms\n",
      "Speed: 3.0ms preprocess, 213.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 230.0ms\n",
      "Speed: 2.0ms preprocess, 230.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 230.3ms\n",
      "Speed: 2.0ms preprocess, 230.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 221.0ms\n",
      "Speed: 1.8ms preprocess, 221.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.34130]), Coordinates: tensor([[196., 122., 215., 176.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 254.0ms\n",
      "Speed: 2.0ms preprocess, 254.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 210.9ms\n",
      "Speed: 1.0ms preprocess, 210.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 251.0ms\n",
      "Speed: 3.0ms preprocess, 251.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 212.7ms\n",
      "Speed: 2.0ms preprocess, 212.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 234.9ms\n",
      "Speed: 3.0ms preprocess, 234.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 239.4ms\n",
      "Speed: 3.0ms preprocess, 239.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.29400]), Coordinates: tensor([[240., 236., 261., 315.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 240.5ms\n",
      "Speed: 3.0ms preprocess, 240.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.30094]), Coordinates: tensor([[242., 242., 262., 313.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 234.1ms\n",
      "Speed: 3.2ms preprocess, 234.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 233.3ms\n",
      "Speed: 3.0ms preprocess, 233.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 214.5ms\n",
      "Speed: 2.0ms preprocess, 214.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 200.9ms\n",
      "Speed: 2.1ms preprocess, 200.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 242.8ms\n",
      "Speed: 2.5ms preprocess, 242.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 217.6ms\n",
      "Speed: 2.0ms preprocess, 217.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 237.3ms\n",
      "Speed: 2.0ms preprocess, 237.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 214.6ms\n",
      "Speed: 2.0ms preprocess, 214.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 238.1ms\n",
      "Speed: 2.0ms preprocess, 238.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 204.5ms\n",
      "Speed: 1.0ms preprocess, 204.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 207.2ms\n",
      "Speed: 11.0ms preprocess, 207.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 232.7ms\n",
      "Speed: 2.0ms preprocess, 232.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.25365]), Coordinates: tensor([[258., 234., 277., 308.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 195.9ms\n",
      "Speed: 2.0ms preprocess, 195.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 211.8ms\n",
      "Speed: 2.0ms preprocess, 211.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 223.2ms\n",
      "Speed: 3.0ms preprocess, 223.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 207.7ms\n",
      "Speed: 1.0ms preprocess, 207.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 233.8ms\n",
      "Speed: 3.0ms preprocess, 233.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 194.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.25283]), Coordinates: tensor([[260., 188., 275., 237.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.0ms preprocess, 194.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 214.4ms\n",
      "Speed: 4.2ms preprocess, 214.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 DOBs, 196.7ms\n",
      "Speed: 2.0ms preprocess, 196.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.34206]), Coordinates: tensor([[259., 254., 273., 282.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.31919]), Coordinates: tensor([[244., 180., 260., 232.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 211.4ms\n",
      "Speed: 1.0ms preprocess, 211.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.27941]), Coordinates: tensor([[260., 180., 272., 226.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 DOBs, 219.0ms\n",
      "Speed: 3.0ms preprocess, 219.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.59431]), Coordinates: tensor([[228., 170., 243., 227.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.52349]), Coordinates: tensor([[248., 180., 260., 222.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 218.0ms\n",
      "Speed: 2.5ms preprocess, 218.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 DOBs, 201.9ms\n",
      "Speed: 1.0ms preprocess, 201.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.35692]), Coordinates: tensor([[245., 221., 260., 272.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.25004]), Coordinates: tensor([[211., 217., 224., 270.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 226.2ms\n",
      "Speed: 2.0ms preprocess, 226.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 192.0ms\n",
      "Speed: 2.0ms preprocess, 192.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 227.2ms\n",
      "Speed: 1.0ms preprocess, 227.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 229.6ms\n",
      "Speed: 3.0ms preprocess, 229.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 198.3ms\n",
      "Speed: 2.1ms preprocess, 198.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 238.2ms\n",
      "Speed: 2.0ms preprocess, 238.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 DOBs, 223.1ms\n",
      "Speed: 2.4ms preprocess, 223.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.54916]), Coordinates: tensor([[229., 177., 242., 228.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.26916]), Coordinates: tensor([[227., 243., 241., 300.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 226.7ms\n",
      "Speed: 2.0ms preprocess, 226.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 198.7ms\n",
      "Speed: 1.1ms preprocess, 198.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 217.0ms\n",
      "Speed: 2.4ms preprocess, 217.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 229.1ms\n",
      "Speed: 3.0ms preprocess, 229.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 235.7ms\n",
      "Speed: 2.0ms preprocess, 235.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 230.3ms\n",
      "Speed: 1.5ms preprocess, 230.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 212.6ms\n",
      "Speed: 2.5ms preprocess, 212.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 232.8ms\n",
      "Speed: 1.0ms preprocess, 232.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 213.3ms\n",
      "Speed: 1.0ms preprocess, 213.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.29640]), Coordinates: tensor([[284., 185., 294., 221.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 275.6ms\n",
      "Speed: 3.0ms preprocess, 275.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.35774]), Coordinates: tensor([[283., 181., 294., 219.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 195.3ms\n",
      "Speed: 1.0ms preprocess, 195.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.28258]), Coordinates: tensor([[285., 240., 298., 286.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 206.5ms\n",
      "Speed: 2.0ms preprocess, 206.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 215.2ms\n",
      "Speed: 2.0ms preprocess, 215.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 203.7ms\n",
      "Speed: 1.0ms preprocess, 203.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 228.0ms\n",
      "Speed: 1.0ms preprocess, 228.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 219.4ms\n",
      "Speed: 4.6ms preprocess, 219.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 219.8ms\n",
      "Speed: 3.0ms preprocess, 219.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.25126]), Coordinates: tensor([[245., 181., 257., 234.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 DOBs, 226.8ms\n",
      "Speed: 3.0ms preprocess, 226.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.37963]), Coordinates: tensor([[277., 176., 287., 213.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.26112]), Coordinates: tensor([[290., 179., 300., 212.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 DOBs, 206.3ms\n",
      "Speed: 2.0ms preprocess, 206.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.30233]), Coordinates: tensor([[288., 237., 301., 283.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.27180]), Coordinates: tensor([[275., 176., 287., 215.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 231.7ms\n",
      "Speed: 2.0ms preprocess, 231.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 223.6ms\n",
      "Speed: 4.0ms preprocess, 223.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.33409]), Coordinates: tensor([[274., 176., 285., 214.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 257.3ms\n",
      "Speed: 3.0ms preprocess, 257.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.27677]), Coordinates: tensor([[288., 239., 302., 284.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 227.0ms\n",
      "Speed: 3.0ms preprocess, 227.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.27340]), Coordinates: tensor([[257., 238., 270., 287.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 264.5ms\n",
      "Speed: 4.0ms preprocess, 264.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.26568]), Coordinates: tensor([[288., 239., 303., 285.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 249.5ms\n",
      "Speed: 1.0ms preprocess, 249.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.25866]), Coordinates: tensor([[259., 238., 272., 287.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 DOBs, 207.8ms\n",
      "Speed: 2.0ms preprocess, 207.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.30692]), Coordinates: tensor([[288., 240., 302., 285.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.27311]), Coordinates: tensor([[277., 178., 288., 220.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 264.4ms\n",
      "Speed: 3.2ms preprocess, 264.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.40243]), Coordinates: tensor([[285., 242., 300., 287.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 208.7ms\n",
      "Speed: 2.0ms preprocess, 208.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 219.7ms\n",
      "Speed: 3.0ms preprocess, 219.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 202.9ms\n",
      "Speed: 1.5ms preprocess, 202.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 205.1ms\n",
      "Speed: 2.0ms preprocess, 205.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 237.6ms\n",
      "Speed: 1.0ms preprocess, 237.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 203.1ms\n",
      "Speed: 2.0ms preprocess, 203.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 214.1ms\n",
      "Speed: 2.0ms preprocess, 214.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 219.0ms\n",
      "Speed: 3.0ms preprocess, 219.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 205.6ms\n",
      "Speed: 2.0ms preprocess, 205.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 244.9ms\n",
      "Speed: 2.0ms preprocess, 244.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 219.5ms\n",
      "Speed: 2.0ms preprocess, 219.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.27121]), Coordinates: tensor([[238., 182., 249., 228.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 208.0ms\n",
      "Speed: 2.0ms preprocess, 208.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 218.3ms\n",
      "Speed: 3.2ms preprocess, 218.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 242.3ms\n",
      "Speed: 2.0ms preprocess, 242.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.29523]), Coordinates: tensor([[255., 178., 265., 214.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 262.7ms\n",
      "Speed: 2.0ms preprocess, 262.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 DOBs, 222.4ms\n",
      "Speed: 2.0ms preprocess, 222.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.62246]), Coordinates: tensor([[258., 178., 268., 215.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.35866]), Coordinates: tensor([[242., 179., 253., 220.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.32646]), Coordinates: tensor([[227., 179., 238., 236.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 DOBs, 265.0ms\n",
      "Speed: 3.0ms preprocess, 265.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.54237]), Coordinates: tensor([[242., 179., 255., 221.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.47650]), Coordinates: tensor([[259., 177., 270., 216.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 DOBs, 221.1ms\n",
      "Speed: 2.0ms preprocess, 221.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.78583]), Coordinates: tensor([[229., 179., 240., 229.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.62378]), Coordinates: tensor([[262., 172., 273., 215.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.29636]), Coordinates: tensor([[229., 175., 239., 247.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.28686]), Coordinates: tensor([[245., 178., 258., 219.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 DOBs, 231.1ms\n",
      "Speed: 10.3ms preprocess, 231.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 DOBs, 189.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.34513]), Coordinates: tensor([[229., 179., 240., 243.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.28634]), Coordinates: tensor([[245., 177., 257., 220.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.25826]), Coordinates: tensor([[229., 174., 239., 280.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.0ms preprocess, 189.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.53790]), Coordinates: tensor([[228., 176., 240., 258.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.42654]), Coordinates: tensor([[229., 178., 241., 234.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.32837]), Coordinates: tensor([[262., 168., 274., 213.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 DOBs, 200.0ms\n",
      "Speed: 1.0ms preprocess, 200.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.48082]), Coordinates: tensor([[229., 177., 241., 226.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.47528]), Coordinates: tensor([[229., 174., 240., 237.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.30752]), Coordinates: tensor([[247., 177., 257., 215.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.25955]), Coordinates: tensor([[263., 170., 273., 213.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 DOBs, 210.6ms\n",
      "Speed: 3.0ms preprocess, 210.6ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.72838]), Coordinates: tensor([[228., 178., 241., 271.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.51193]), Coordinates: tensor([[229., 179., 241., 242.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 DOBs, 186.0ms\n",
      "Speed: 2.0ms preprocess, 186.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.82410]), Coordinates: tensor([[229., 179., 242., 227.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.50733]), Coordinates: tensor([[229., 177., 242., 257.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.29615]), Coordinates: tensor([[229., 174., 243., 296.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 DOBs, 193.1ms\n",
      "Speed: 1.0ms preprocess, 193.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.90120]), Coordinates: tensor([[229., 179., 241., 229.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.39118]), Coordinates: tensor([[229., 176., 240., 245.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.31161]), Coordinates: tensor([[263., 168., 274., 213.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.31017]), Coordinates: tensor([[247., 179., 258., 217.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 DOBs, 194.9ms\n",
      "Speed: 1.0ms preprocess, 194.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.36034]), Coordinates: tensor([[230., 179., 242., 265.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.29038]), Coordinates: tensor([[245., 235., 260., 282.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 DOBs, 202.7ms\n",
      "Speed: 2.0ms preprocess, 202.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.78704]), Coordinates: tensor([[229., 178., 241., 228.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.26019]), Coordinates: tensor([[245., 178., 259., 220.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.25976]), Coordinates: tensor([[229., 173., 240., 292.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 DOBs, 218.6ms\n",
      "Speed: 2.0ms preprocess, 218.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.57169]), Coordinates: tensor([[230., 178., 242., 238.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.35327]), Coordinates: tensor([[248., 172., 260., 217.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.32616]), Coordinates: tensor([[265., 174., 276., 213.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.30611]), Coordinates: tensor([[229., 174., 243., 275.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 190.1ms\n",
      "Speed: 1.7ms preprocess, 190.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.31803]), Coordinates: tensor([[271., 235., 286., 285.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 200.8ms\n",
      "Speed: 1.0ms preprocess, 200.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 216.5ms\n",
      "Speed: 1.5ms preprocess, 216.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 216.4ms\n",
      "Speed: 0.7ms preprocess, 216.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.27641]), Coordinates: tensor([[240., 193., 256., 260.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 186.2ms\n",
      "Speed: 1.0ms preprocess, 186.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 246.2ms\n",
      "Speed: 2.0ms preprocess, 246.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 183.6ms\n",
      "Speed: 2.0ms preprocess, 183.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 244.1ms\n",
      "Speed: 1.0ms preprocess, 244.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 187.3ms\n",
      "Speed: 1.5ms preprocess, 187.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 214.9ms\n",
      "Speed: 1.0ms preprocess, 214.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Name, 221.7ms\n",
      "Speed: 3.0ms preprocess, 221.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.33343]), Coordinates: tensor([[514., 208., 533., 333.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 201.0ms\n",
      "Speed: 1.0ms preprocess, 201.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 212.2ms\n",
      "Speed: 2.0ms preprocess, 212.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 232.3ms\n",
      "Speed: 2.0ms preprocess, 232.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 191.7ms\n",
      "Speed: 1.0ms preprocess, 191.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.25684]), Coordinates: tensor([[209., 244., 231., 324.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 241.6ms\n",
      "Speed: 2.0ms preprocess, 241.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.27061]), Coordinates: tensor([[210., 235., 231., 323.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 241.3ms\n",
      "Speed: 2.0ms preprocess, 241.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 1 Name, 216.5ms\n",
      "Speed: 2.0ms preprocess, 216.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.28742]), Coordinates: tensor([[512., 209., 532., 325.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.28545]), Coordinates: tensor([[207., 233., 230., 320.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 239.4ms\n",
      "Speed: 2.0ms preprocess, 239.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 1 Name, 192.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.40372]), Coordinates: tensor([[209., 234., 231., 321.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.8ms preprocess, 192.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.31043]), Coordinates: tensor([[514., 210., 533., 330.]])\n",
      "Class: tensor([0.]), Confidence: tensor([0.25600]), Coordinates: tensor([[211., 233., 231., 320.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Name, 233.2ms\n",
      "Speed: 1.0ms preprocess, 233.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.37986]), Coordinates: tensor([[514., 209., 534., 334.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 206.9ms\n",
      "Speed: 2.0ms preprocess, 206.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Name, 247.1ms\n",
      "Speed: 3.0ms preprocess, 247.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 196.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.25982]), Coordinates: tensor([[597., 223., 625., 413.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.0ms preprocess, 196.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 198.1ms\n",
      "Speed: 2.6ms preprocess, 198.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 205.9ms\n",
      "Speed: 1.0ms preprocess, 205.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 199.1ms\n",
      "Speed: 2.2ms preprocess, 199.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Name, 218.3ms\n",
      "Speed: 1.4ms preprocess, 218.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.30810]), Coordinates: tensor([[518., 214., 542., 335.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 217.2ms\n",
      "Speed: 3.0ms preprocess, 217.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 DOB, 197.0ms\n",
      "Speed: 1.5ms preprocess, 197.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.29624]), Coordinates: tensor([[254., 135., 276., 201.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Name, 206.7ms\n",
      "Speed: 2.0ms preprocess, 206.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.30508]), Coordinates: tensor([[519., 217., 541., 329.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 200.3ms\n",
      "Speed: 2.5ms preprocess, 200.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.31545]), Coordinates: tensor([[226., 242., 245., 319.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 191.4ms\n",
      "Speed: 2.0ms preprocess, 191.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.25369]), Coordinates: tensor([[286., 237., 309., 324.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 226.5ms\n",
      "Speed: 3.0ms preprocess, 226.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.28066]), Coordinates: tensor([[286., 237., 307., 319.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 DOB, 232.8ms\n",
      "Speed: 2.0ms preprocess, 232.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([0.]), Confidence: tensor([0.25569]), Coordinates: tensor([[288., 235., 310., 318.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 226.6ms\n",
      "Speed: 2.1ms preprocess, 226.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 212.0ms\n",
      "Speed: 2.0ms preprocess, 212.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Name, 230.1ms\n",
      "Speed: 2.0ms preprocess, 230.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.29197]), Coordinates: tensor([[527., 214., 546., 324.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 229.1ms\n",
      "Speed: 1.4ms preprocess, 229.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 184.1ms\n",
      "Speed: 1.0ms preprocess, 184.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 218.4ms\n",
      "Speed: 2.0ms preprocess, 218.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 195.8ms\n",
      "Speed: 2.0ms preprocess, 195.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 212.8ms\n",
      "Speed: 1.5ms preprocess, 212.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 198.8ms\n",
      "Speed: 1.0ms preprocess, 198.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Name, 227.0ms\n",
      "Speed: 2.0ms preprocess, 227.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tensor([2.]), Confidence: tensor([0.28391]), Coordinates: tensor([[520., 209., 542., 321.]])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def draw_guide_box(frame, box_coords):\n",
    "    x, y, w, h = box_coords\n",
    "\n",
    "    # Draw the guide box\n",
    "    color = (0, 255, 0)  # Default color: Green\n",
    "    thickness = 2\n",
    "\n",
    "    # Define the region where the card is expected\n",
    "    roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "    # Check if the object is inside the guide box\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "    if edges.mean() < 5:  # Simple check for presence (modify as needed for precision)\n",
    "        color = (0, 0, 255)  # Change to red if the object is not detected in the region\n",
    "        cv2.putText(frame, \"Move card inside the box!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Draw the rectangle\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), color, thickness)\n",
    "    return frame\n",
    "\n",
    "def main():\n",
    "    # Initialize video capture (webcam)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Coordinates for the guide box (center of the frame)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    box_width, box_height = 300, 200  # Define the size of the box\n",
    "    box_x = (frame_width - box_width) // 2\n",
    "    box_y = (frame_height - box_height) // 2\n",
    "    box_coords = (box_x, box_y, box_width, box_height)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Draw the guide box and provide feedback\n",
    "        updated_frame = draw_guide_box(frame, box_coords)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Card Placement Guide\", updated_frame)\n",
    "\n",
    "        results = model5.predict(source=frame, conf=0.25)\n",
    "\n",
    "        # Render the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        cv2.imshow(\"Annotated Frame\", annotated_frame)  \n",
    "        for box in results[0].boxes:\n",
    "            print(f\"Class: {box.cls}, Confidence: {box.conf}, Coordinates: {box.xyxy}\")\n",
    "\n",
    "        # Exit loop on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the best-performing model\n",
    "model5 = YOLO('runs/detect/train15/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.43  Python-3.7.3 torch-1.13.1+cpu CPU\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "0: 480x640 (no detections), 223.4ms\n",
      "Speed: 6.5ms preprocess, 223.4ms inference, 16.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 181.4ms\n",
      "Speed: 2.0ms preprocess, 181.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 160.6ms\n",
      "Speed: 0.0ms preprocess, 160.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n",
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 185.9ms\n",
      "Speed: 2.0ms preprocess, 185.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 167.5ms\n",
      "Speed: 1.9ms preprocess, 167.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n",
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 218.7ms\n",
      "Speed: 0.0ms preprocess, 218.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 164.5ms\n",
      "Speed: 0.0ms preprocess, 164.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n",
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 177.3ms\n",
      "Speed: 1.0ms preprocess, 177.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 176.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.0ms preprocess, 176.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 171.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 171.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 175.6ms\n",
      "Speed: 1.5ms preprocess, 175.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 153.6ms\n",
      "Speed: 0.0ms preprocess, 153.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n",
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 160.2ms\n",
      "Speed: 0.0ms preprocess, 160.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 178.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 178.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 185.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.5ms preprocess, 185.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 188.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.8ms preprocess, 188.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 159.1ms\n",
      "Speed: 0.0ms preprocess, 159.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n",
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 170.0ms\n",
      "Speed: 1.6ms preprocess, 170.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 166.5ms\n",
      "Speed: 1.4ms preprocess, 166.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 147.5ms\n",
      "Speed: 1.0ms preprocess, 147.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n",
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 174.4ms\n",
      "Speed: 0.0ms preprocess, 174.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 172.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 172.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 171.4ms\n",
      "Speed: 1.0ms preprocess, 171.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 181.9ms\n",
      "Speed: 2.6ms preprocess, 181.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 156.3ms\n",
      "Speed: 1.3ms preprocess, 156.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n",
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 171.0ms\n",
      "Speed: 3.4ms preprocess, 171.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 162.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 4.4ms preprocess, 162.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 198.1ms\n",
      "Speed: 3.3ms preprocess, 198.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 175.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 175.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 166.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 4.4ms preprocess, 166.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 231.5ms\n",
      "Speed: 2.8ms preprocess, 231.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 179.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 179.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 199.9ms\n",
      "Speed: 1.0ms preprocess, 199.9ms inference, 10.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 183.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 183.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 179.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.0ms preprocess, 179.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 177.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.0ms preprocess, 177.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 153.5ms\n",
      "Speed: 2.0ms preprocess, 153.5ms inference, 15.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 166.3ms\n",
      "Speed: 0.0ms preprocess, 166.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n",
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 173.3ms\n",
      "Speed: 0.0ms preprocess, 173.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 163.6ms\n",
      "Speed: 0.0ms preprocess, 163.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n",
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 159.9ms\n",
      "Speed: 0.0ms preprocess, 159.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 150.6ms\n",
      "Speed: 2.0ms preprocess, 150.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: {'Pan Number', 'Income Logo', 'DOB', 'Name'}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "expected_classes = ['DOB', 'Income Logo', 'Name', 'Pan Number']\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for webcam, or replace with video file path\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run the model on the frame\n",
    "    results = model5.predict(source=frame, conf=0.25)\n",
    "    detected_classes = set()\n",
    "    for result in results:  # Assuming batch size 1\n",
    "        for box in result.boxes:  # Access the detected boxes\n",
    "            class_id = int(box.cls)  # Extract class ID\n",
    "            detected_classes.add(model5.names[class_id]) \n",
    "\n",
    "    # Check if all expected classes are detected\n",
    "    missing_classes = set(expected_classes) - detected_classes\n",
    "    if missing_classes:\n",
    "        print(\"Missing classes:\", missing_classes)\n",
    "    else:\n",
    "        print(\"All classes detected.\")\n",
    "\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    cv2.imshow(\"Annotated Frame\", annotated_frame)  \n",
    "    for box in results[0].boxes:\n",
    "        print(f\"Class: {box.cls}, Confidence: {box.conf}, Coordinates: {box.xyxy}\")\n",
    "\n",
    "    # Exit loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the best-performing model\n",
    "model6 = YOLO('runs/detect/train5/weights/best.pt')\n",
    "model7 = YOLO('runs/detect/train7/weights/best.pt')\n",
    "model5 = YOLO('runs/detect/train15/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Detecting first set of classes...\n",
      "Step 1 Failed: Missing classes in Model 1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "model6 = YOLO('runs/detect/train5/weights/best.pt')\n",
    "model7 = YOLO('runs/detect/train7/weights/best.pt')\n",
    "model5 = YOLO('runs/detect/train15/weights/best.pt')\n",
    "# # Load YOLO models\n",
    "# model1 = YOLO(\"model1.pt\")  # First YOLO model to detect 2 classes\n",
    "# model2 = YOLO(\"model2.pt\")  # Second YOLO model to detect 4 classes\n",
    "\n",
    "def detect_with_model(model, frame, required_classes):\n",
    "    results = model.predict(source=frame, conf=0.25, save=False, verbose=False)\n",
    "    detected_classes = set()\n",
    "\n",
    "    # Parse results to extract detected class IDs\n",
    "    for box in results[0].boxes:\n",
    "        class_id = int(box.cls)\n",
    "        detected_classes.add(model.names[class_id])\n",
    "\n",
    "    # Check if all required classes are detected\n",
    "    all_classes_detected = required_classes.issubset(detected_classes)\n",
    "    return detected_classes, all_classes_detected\n",
    "\n",
    "def main():\n",
    "    # Define required classes for each step\n",
    "    required_classes_model1 = {\"PIC\", \"Satyamav Jayate\"}  # Replace with actual class names\n",
    "    required_classes_model2 = {\"DOB\", \"Income Logo\", \"Name\", \"Pan Number\"}\n",
    "    required_classes_model3 = {\"GOV\", \"Name\", \"DOB\", \"Gender\"}\n",
    "\n",
    "    # Step 1: Open webcam for detecting first set of classes\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    print(\"Step 1: Detecting first set of classes...\")\n",
    "    step1_success = False\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Detect with Model 1\n",
    "        detected_classes_1, step1_success = detect_with_model(model6, frame, required_classes_model1)\n",
    "        annotated_frame = model6.predict(source=frame, conf=0.25, save=False)[0].plot()\n",
    "        cv2.imshow(\"Step 1: Detecting First Set\", annotated_frame)\n",
    "\n",
    "        if step1_success:\n",
    "            print(\"Step 1 Success: Detected Classes (Model 1):\", detected_classes_1)\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "        # Exit on pressing 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    if not step1_success:\n",
    "        print(\"Step 1 Failed: Missing classes in Model 1\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Open webcam for detecting second set of classes\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    print(\"Step 2: Detecting second set of classes...\")\n",
    "    step2_success = False\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Detect with Model 2\n",
    "        detected_classes_2, step2_success = detect_with_model(model5, frame, required_classes_model2)\n",
    "        annotated_frame = model5.predict(source=frame, conf=0.25, save=False)[0].plot()\n",
    "        cv2.imshow(\"Step 2: Detecting Second Set\", annotated_frame)\n",
    "\n",
    "        if step2_success:\n",
    "            print(\"Step 2 Success: Detected Classes (Model 2):\", detected_classes_2)\n",
    "            print(\"All classes detected successfully!\")\n",
    "            print(\"This is a Pan Card\")\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "        # Exit on pressing 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    if not step2_success:\n",
    "        print(\"Step 2 Failed: Missing classes in Model 2\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Detecting first set of classes...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model6' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_46596\\2499174005.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_46596\\2499174005.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# Detect with Model 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mdetected_classes_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep1_success\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_with_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequired_classes_model1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mannotated_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Step 1: Detecting First Set\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotated_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model6' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def detect_with_model(model, frame, required_classes):\n",
    "    results = model.predict(source=frame, conf=0.25, save=False, verbose=False)\n",
    "    detected_classes = set()\n",
    "\n",
    "    # Parse results to extract detected class IDs\n",
    "    for box in results[0].boxes:\n",
    "        class_id = int(box.cls)\n",
    "        detected_classes.add(model.names[class_id])\n",
    "\n",
    "    # Check if all required classes are detected\n",
    "    all_classes_detected = required_classes.issubset(detected_classes)\n",
    "    return detected_classes, all_classes_detected\n",
    "\n",
    "def main():\n",
    "    # Define required classes for each step\n",
    "    required_classes_model1 = {\"PIC\", \"Satyamav Jayate\"}  # Replace with actual class names\n",
    "    required_classes_model3 = {\"GOV\", \"Name\", \"DOB\", \"Gender\"}\n",
    "\n",
    "    # Step 1: Open webcam for detecting first set of classes\n",
    "    cap = cv2.VideoCapture(1)\n",
    "\n",
    "    print(\"Step 1: Detecting first set of classes...\")\n",
    "    step1_success = False\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Detect with Model 1\n",
    "        detected_classes_1, step1_success = detect_with_model(model6, frame, required_classes_model1)\n",
    "        annotated_frame = model6.predict(source=frame, conf=0.25, save=False)[0].plot()\n",
    "        cv2.imshow(\"Step 1: Detecting First Set\", annotated_frame)\n",
    "\n",
    "        if step1_success:\n",
    "            print(\"Step 1 Success: Detected Classes (Model 1):\", detected_classes_1)\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "        # Exit on pressing 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    if not step1_success:\n",
    "        print(\"Step 1 Failed: Missing classes in Model 1\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Open webcam for detecting second set of classes\n",
    "    cap = cv2.VideoCapture(1)\n",
    "\n",
    "    print(\"Step 2: Detecting second set of classes...\")\n",
    "    step2_success = False\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Detect with Model 2\n",
    "        detected_classes_3, step2_success = detect_with_model(model7, frame, required_classes_model3)\n",
    "        annotated_frame = model7.predict(source=frame, conf=0.25, save=False)[0].plot()\n",
    "        cv2.imshow(\"Step 2: Detectin\\g Second Set\", annotated_frame)\n",
    "\n",
    "        if step2_success:\n",
    "            print(\"Step 2 Success: Detected Classes (Model 2):\", detected_classes_3)\n",
    "            print(\"All classes detected successfully!\")\n",
    "            print(\"This is a Aadhar Card\")\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "        # Exit on pressing 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    if not step2_success:\n",
    "        print(\"Step 2 Failed: Missing classes in Model 2\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting folium\n",
      "  Downloading folium-0.19.4-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting opencage\n",
      "  Using cached opencage-3.0.4-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.9.1)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.13)\n",
      "Collecting branca>=0.6.0 (from folium)\n",
      "  Using cached branca-0.8.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from folium) (3.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from folium) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from folium) (2.32.3)\n",
      "Collecting xyzservices (from folium)\n",
      "  Downloading xyzservices-2025.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting backoff>=2.2.1 (from opencage)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.4 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opencage) (4.66.5)\n",
      "Requirement already satisfied: certifi>=2024.07.04 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opencage) (2024.7.4)\n",
      "Requirement already satisfied: aiohttp>=3.10.5 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opencage) (3.11.11)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp>=3.10.5->opencage) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp>=3.10.5->opencage) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp>=3.10.5->opencage) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp>=3.10.5->opencage) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp>=3.10.5->opencage) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp>=3.10.5->opencage) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp>=3.10.5->opencage) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2>=2.9->folium) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->folium) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->folium) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->folium) (2.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kiit0001\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.66.4->opencage) (0.4.6)\n",
      "Downloading folium-0.19.4-py2.py3-none-any.whl (110 kB)\n",
      "Downloading opencage-3.0.4-py3-none-any.whl (22 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached branca-0.8.1-py3-none-any.whl (26 kB)\n",
      "Downloading xyzservices-2025.1.0-py3-none-any.whl (88 kB)\n",
      "Installing collected packages: xyzservices, backoff, branca, opencage, folium\n",
      "Successfully installed backoff-2.2.1 branca-0.8.1 folium-0.19.4 opencage-3.0.4 xyzservices-2025.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install folium opencage matplotlib pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geocoded Location: Latitude=20.27241, Longitude=85.83385\n",
      "Map saved as map.html\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# import pytesseract\n",
    "from opencage.geocoder import OpenCageGeocode\n",
    "import folium\n",
    "\n",
    "# Initialize the geocoder with your API key\n",
    "API_KEY = \"644f60b1b6214fb281833b633791a424\"\n",
    "geocoder = OpenCageGeocode(API_KEY)\n",
    "\n",
    "# Function to extract text (address) using OCR\n",
    "# def extract_address(image_path):\n",
    "#     # Read the image\n",
    "#     image = cv2.imread(image_path)\n",
    "#     # Convert image to grayscale\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     # Perform OCR\n",
    "#     text = pytesseract.image_to_string(gray)\n",
    "#     # Simulate extracting address (filtering out non-address parts)\n",
    "#     address = text.split(\"\\n\")[-1]  # Assuming the last line is the address\n",
    "#     return address\n",
    "\n",
    "# Function to geocode address to latitude and longitude\n",
    "def geocode_address(address):\n",
    "    results = geocoder.geocode(address)\n",
    "    if results:\n",
    "        lat = results[0][\"geometry\"][\"lat\"]\n",
    "        lng = results[0][\"geometry\"][\"lng\"]\n",
    "        return lat, lng\n",
    "    return None, None\n",
    "\n",
    "# Function to create a map and plot the location\n",
    "def plot_on_map(lat, lng, address):\n",
    "    # Create a map centered at the given location\n",
    "    map_ = folium.Map(location=[lat, lng], zoom_start=15)\n",
    "    # Add a marker for the location\n",
    "    folium.Marker([lat, lng], popup=address).add_to(map_)\n",
    "    # Save the map to an HTML file\n",
    "    map_.save(\"map.html\")\n",
    "    print(\"Map saved as map.html\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Step 1: Extract address from Aadhaar card\n",
    "    # address = extract_address(image_path)\n",
    "    # print(f\"Extracted Address: {address}\")\n",
    "\n",
    "    # Step 2: Geocode the address\n",
    "    lat, lng = geocode_address(\"Ashirvad Ashraya, Patia, Bhubaneswar, Odisha, India\")\n",
    "    if lat and lng:\n",
    "        print(f\"Geocoded Location: Latitude={lat}, Longitude={lng}\")\n",
    "\n",
    "        # Step 3: Plot the location on a map\n",
    "        plot_on_map(lat, lng, \"Ashirvad Ashraya, Patia, Bhubaneswar, Odisha, India\")\n",
    "    else:\n",
    "        print(\"Failed to geocode the address.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example Aadhaar card image\n",
    "    # aadhaar_card_image = \"aadhaar_card_sample.jpg\"\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-visionNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading google_cloud_vision-3.9.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision)\n",
      "  Downloading google_api_core-2.24.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from google-cloud-vision) (2.37.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-cloud-vision)\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from google-cloud-vision) (3.20.3)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.62.3)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision)\n",
      "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (4.9)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 (from google-cloud-vision)\n",
      "  Downloading protobuf-4.24.4-cp37-cp37m-win_amd64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2024.8.30)\n",
      "Downloading google_cloud_vision-3.9.0-py2.py3-none-any.whl (514 kB)\n",
      "   ---------------------------------------- 514.6/514.6 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.24.0-py3-none-any.whl (158 kB)\n",
      "   ---------------------------------------- 158.6/158.6 kB 9.3 MB/s eta 0:00:00\n",
      "Downloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 50.1/50.1 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "   ---------------------------------------- 221.7/221.7 kB 6.8 MB/s eta 0:00:00\n",
      "Downloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Downloading protobuf-4.24.4-cp37-cp37m-win_amd64.whl (430 kB)\n",
      "   --------------------------------------- 430.0/430.0 kB 13.1 MB/s eta 0:00:00\n",
      "Installing collected packages: protobuf, proto-plus, googleapis-common-protos, grpcio-status, google-api-core, google-cloud-vision\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "Successfully installed google-api-core-2.24.0 google-cloud-vision-3.9.0 googleapis-common-protos-1.66.0 grpcio-status-1.62.3 proto-plus-1.25.0 protobuf-4.24.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.24.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-cloud-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected text: ଭାରତ ସରକାର\n",
      "Government of India\n",
      "ରୋହନ କୁମାର ଦାସ\n",
      "Rohan Kumar Das\n",
      "G/DOB: 18/07/2005\n",
      "ପୁରୁଷ/ MALE\n",
      "Download Date: 07/11/2021\n",
      "ଆଧାର\n",
      "Issue Date: 18/10/2021\n",
      "Ga\n",
      "6313 0626 5686\n",
      "VID: 9139 8841 5639 2617\n",
      "Aadhaar number detected: 6313 0626 5686\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import vision\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Set the path to your credentials\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"C:\\Users\\KIIT0001\\ocr\\ocr_id\\samples\\new_preprocessed\\ocr-recognition-440606-b6f682bf20bb.json\"\n",
    "\n",
    "# Initialize Vision API client\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "# Test text detection\n",
    "image_path = \"adhar.jpg\"\n",
    "with open(image_path, \"rb\") as image_file:\n",
    "    content = image_file.read()\n",
    "    image = vision.Image(content=content)\n",
    "\n",
    "aadhaar_pattern = r\"\\b[2-9]{1}[0-9]{3}\\s?[0-9]{4}\\s?[0-9]{4}\\b\"\n",
    "response = client.text_detection(image=image)\n",
    "if response.text_annotations:\n",
    "    print(\"Detected text:\", response.text_annotations[0].description)\n",
    "    matches = re.findall(aadhaar_pattern, response.text_annotations[0].description)\n",
    "    if matches:\n",
    "        print(\"Aadhaar number detected:\", matches[0])\n",
    "else:\n",
    "    print(\"No text detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (4.10.0.84)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\kiit0001\\ocr\\ocrd\\lib\\site-packages (from opencv-python) (1.21.6)\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (39.4 MB)\n",
      "   ---------------------------------------- 39.4/39.4 MB 15.6 MB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python-headless\n",
      "Successfully installed opencv-python-headless-4.11.0.86\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open the IP camera stream.\n",
      "Failed to capture frame. Reconnecting...\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1295: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11248\\4044847069.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1295: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from IPython.display import display, clear_output\n",
    "import PIL.Image\n",
    "\n",
    "# Set the IP camera's URL\n",
    "# camera_url = \"http://192.168.29.10:4747\"  # Replace with the actual IP camera URL\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Failed to open the IP camera stream.\")\n",
    "else:\n",
    "    print(\"Camera stream opened successfully.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture frame. Reconnecting...\")\n",
    "            break\n",
    "\n",
    "        # Display the frame in Jupyter Notebook\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = PIL.Image.fromarray(img)\n",
    "        clear_output(wait=True)\n",
    "        display(img)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stream stopped by the user.\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"http://192.168.29.10:4747/video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocrd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
